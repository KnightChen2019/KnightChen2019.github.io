<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>这！就是对抗生成网络（四十六）</title>
      <link href="/2019/09/29/dl46/"/>
      <url>/2019/09/29/dl46/</url>
      
        <content type="html"><![CDATA[<h5 id="什么是GAN？"><a href="#什么是GAN？" class="headerlink" title="什么是GAN？"></a>什么是GAN？</h5><p>对抗生成网络（Generative Adversarial Nets，GAN）是由lan Goodfellow在2014年提出来的，它是一种生成式模型，属于无监督学习。先看一张GAN的图片：</p><p><img src="/2019/09/29/dl46/dl4601.JPG" alt="GAN"></p><p>GAN主要由生成网络G（Generator Network）和判别网络D（Discriminator Network）构成，生成模型G将一个噪声包装成一个逼真的样本，判别模型D则需要判断送入的样本是真是的还是假的样本，通过这种相互博弈，判别模型D对样本的判别能力不断上升，生成模型G的造假能力也不断上升，当判别模型D无法判别生成模型G生成的图像真伪时，即达到纳什均衡。</p><h5 id="纳什均衡（Nash-Equilibrium）"><a href="#纳什均衡（Nash-Equilibrium）" class="headerlink" title="纳什均衡（Nash Equilibrium）"></a>纳什均衡（Nash Equilibrium）</h5><p>也称作纳什平衡，或者非合作博弈均衡，是博弈论的一个重要术语，以约翰•纳什命名。在一个博弈过程中，不论对方的策略选择如何，当事人一方都会选择某个确定的策略，则该策略被称作支配性策略。如果两个博弈的当事人的策略组合分别构成各自的支配性策略，那么这个组合就被定义为纳什均衡。</p><p>如果在一次博弈中，双方到达了纳什均衡，也就是意味着博弈双方都没有改变自己策略的动力了，因为单方面改变自己的策略都会造成自己收益的减少。那么纳什均衡点就可以理解为个体最优解，而非集体最优解。</p><p>有一个经典的例子叫做囚徒困境。</p><h6 id="囚徒困境"><a href="#囚徒困境" class="headerlink" title="囚徒困境"></a>囚徒困境</h6><p>假设有两个小偷A和B联合犯事，私入民宅被警察抓住。警方将两人分别置于不同的两个房间内进行审讯，对每一个犯罪嫌疑人，警方给出的政策是：如果一个犯罪嫌疑人坦白了罪行，交出了赃物，于是证据确凿，两人都被判有罪。如果另一个犯罪嫌疑人也做了坦白，则两人给被判刑8年；如果另外一个犯罪嫌疑人没有坦白而是抵赖，则以妨碍公务罪（因已有证据表明其有罪）再加刑2年，而坦白者有功被减刑8年，里脊释放。如果两人都抵赖，则警方因证据不足不能判两人的偷窃罪，但可以私入民宅的罪名将两人各判入狱1年，如下图所示：</p><p><img src="/2019/09/29/dl46/dl4602.JPG" alt="囚徒困境"></p><p>对于此案，最好的策略是双方都抵赖，结果是大家都只判刑1年。但是由于两人处于隔离的情况，首先从心理学的角度来看，当事双方都会怀疑对方会出卖自己以求自保，其次才是亚当斯密的理论，假设每个人都是“理性的经纪人”，都会从利己的目的出发进行选择。两人都会有这样一个盘算过程：假如他坦白，如果我抵赖，得坐10年监狱，如果我坦白，最多才8年；假如他抵赖，如果我也抵赖，我就会被判刑1年，如果我坦白，就可以被释放，而他会作10年牢。综合以上几种情况考虑，不管他坦白与否，对我而言，都是坦白了划算。两人都会这样想，最终两人都坦白了，结果都被判刑8年。</p><p>这与我国开车加塞的例子很像。如果大家都不加塞，是整体的最优解，但是按照纳什均衡理论，任何一个司机都会考虑，无论别人是否加塞，我加塞都可以使自己的收益变大。于是最终大家都会加塞，加剧拥堵，反而不如大家都不加塞走的快。</p><p>那么，有没有办法使个人最优变成集体最优呢？方法就是共谋。两个小偷在作案之前可以说好，咱们如果进去了，一定都抗拒。如果你这一次敢反悔，那么以后道上的人再也不会有人跟你一起了。也就是说，在多次博弈过程中，共谋是可能的。但是如果这个小偷想干完这一票就走，共谋就是不牢靠的。</p><p>在社会领域，共谋是靠法律完成的。大家约定的共谋结论就是法律，如果有人不按照约定做，就会受到法律的惩罚。通过这种方式保证最终决策从个人最优的纳什均衡点变为集体最优点。</p><h5 id="GAN-目标函数"><a href="#GAN-目标函数" class="headerlink" title="GAN 目标函数"></a>GAN 目标函数</h5><p>原论文中，GAN的目标函数如下：<br>$$<br>\underset{G}{min} \underset{D}{max}V(D,G) = \mathbb {E}_{x \sim P_{data}(x)}[logD(x)] +\mathbb {E}_{z \sim P_{z}(Z)}[log(1-D(G(z)))]<br>$$<br>其中G表示判别网络，D表示生成网络。</p><p>$D(x)$表示判断真实数据是否真实的概率，x表示的真实数据，所以$D(x)$越接近1越好。</p><p>$G(z)$是生成的假样本，所以$D(G(z))$越接近0越好。</p><p>$\mathbb {E}_{x \sim P_{data}(x)}[logD(x)]$中，E表示期望，判别出x属于真实数据data的对数损失函数。最大化这一项相当于令判别器D在x服从于data的概率密度时能准确的预测$D(x)=1$。</p><p>$\mathbb {E}_{z \sim P_{z}(Z)}[log(1-D(G(z)))]$表示判别器D识别$G(z)$都为0，即$G(z)$没有欺骗判别器D。</p><p>从判别器D的角度看，它希望自己能尽可能的区分真实样本和虚假样本，因此希望$D(x)$尽可能大，$D(G(z))$尽可能小，即$V(D,G)$尽可能大。从生成器G的角度看，它希望自己能可能骗过D，也就是希望$D(G(z))$尽可能大，即$V(D,G)$尽可能小，两个模型对抗，最后达到全局最优。</p><p>可以用下图表示整个过程：</p><p><img src="/2019/09/29/dl46/dl4603.JPG" alt="训练过程"></p><p>黑色曲线是真实样本的概率分布函数，绿色曲线是虚假样本的概率分布函数，蓝色曲线是判别器D的输出，它的值越大表示这个样本越有可能是真实样本，最下方的平行线是噪声z，它映射到了x。</p><p>我们可以看到，一开始$G(z)$和x虽然是在同一个特征空间里，但是它们分布的差异很大，这时，虽然鉴别真实样本和虚假样本的模型D的性能步枪，但它很容易就能把两者区分开来，而随着训练的推进，虚假样本的分布逐渐与真实样本重合，D虽然也在不断更新，但已力不从心了。最后，黑线和绿线几乎重合，模型达到了最有状态，这时D的输出对于任意样本都是0.5。</p><h5 id="GAN训练方法"><a href="#GAN训练方法" class="headerlink" title="GAN训练方法"></a>GAN训练方法</h5><p>贴一张原论文中的算法图：</p><p><img src="/2019/09/29/dl46/dl4604.JPG" alt="GAN算法"></p><ul><li>在噪声数据分布中随机采样，输入生成模型，得到一组假数据，记为$D(z)$。</li><li>在真实数据分布中随机采样，做为真实数据，记为x。</li><li>将前两步中某一步产生的数据作为判别网络的输入（因为判别网络模型的输入为两类数据，真/假），判别网络的输出值为该输入属于真实数据的概率，real为1，fake为0。</li><li>然后根据得到的概率值计算损失函数。</li><li>根据判别模型和生成模型的损失函数，利用反向传播算法，更新模型的参数（先更新判别式模型的参数，然后通过采样得到的噪声数据更细生成器的参数）。</li></ul><h5 id="GAN优缺点-（来自lan-Goodfellow在Quora上的回答）"><a href="#GAN优缺点-（来自lan-Goodfellow在Quora上的回答）" class="headerlink" title="GAN优缺点 （来自lan Goodfellow在Quora上的回答）"></a>GAN优缺点 （来自lan Goodfellow在Quora上的回答）</h5><h6 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h6><ul><li>根据实际的结果，它们看上去可以比其它模型产生了更好的样本（图像更锐利、清晰）。</li><li>生成对抗式网络框架能训练任何一种生成器网络（理论上-实践中，用 REINFORCE 来训练带有离散输出的生成网络非常困难）。大部分其他的框架需要该生成器网络有一些特定的函数形式，比如输出层是高斯的。重要的是所有其他的框架需要生成器网络遍布非零质量（non-zero mass）。</li><li>生成对抗式网络能学习可以仅在与数据接近的细流形（thin manifold）上生成点。</li><li>不需要设计遵循任何种类的因式分解的模型，任何生成器网络和任何鉴别器都会有用。</li><li>无需利用马尔科夫链反复采样，无需在学习过程中进行推断（Inference），回避了近似计算棘手的概率的难题。</li><li>与PixelRNN相比，生成一个样本的运行时间更小。GAN 每次能产生一个样本，而 PixelRNN 需要一次产生一个像素来生成样本。 </li><li>与VAE 相比，它没有变化的下限。如果鉴别器网络能完美适合，那么这个生成器网络会完美地恢复训练分布。换句话说，各种对抗式生成网络会渐进一致（asymptotically consistent），而 VAE 有一定偏置。 </li><li>与深度玻尔兹曼机相比，既没有一个变化的下限，也没有棘手的分区函数。它的样本可以一次性生成，而不是通过反复应用马尔可夫链运算器（Markov chain operator）。 </li><li>与 GSN 相比，它的样本可以一次生成，而不是通过反复应用马尔可夫链运算器。 </li><li>与NICE 和 Real NVE 相比，在 latent code 的大小上没有限制。</li></ul><h6 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h6><ul><li>解决不收敛（non-convergence）的问题。<br>目前面临的基本问题是：所有的理论都认为 GAN 应该在纳什均衡（Nash equilibrium）上有卓越的表现，但梯度下降只有在凸函数的情况下才能保证实现纳什均衡。当博弈双方都由神经网络表示时，在没有实际达到均衡的情况下，让它们永远保持对自己策略的调整是可能的。</li><li>难以训练：崩溃问题（collapse problem）<br>GAN模型被定义为极小极大问题，没有损失函数，在训练过程中很难区分是否正在取得进展。GAN的学习过程可能发生崩溃问题（collapse problem），生成器开始退化，总是生成同样的样本点，无法继续学习。当生成模型崩溃时，判别模型也会对相似的样本点指向相似的方向，训练无法继续。</li><li>无需预先建模，模型过于自由不可控。<br>与其他生成式模型相比，GAN这种竞争的方式不再要求一个假设的数据分布，即不需要formulate p(x)，而是使用一种分布直接进行采样sampling，从而真正达到理论上可以完全逼近真实数据，这也是GAN最大的优势。然而，这种不需要预先建模的方法缺点是太过自由了，对于较大的图片，较多的 pixel的情形，基于简单 GAN 的方式就不太可控了(超高维)。在GAN[Goodfellow Ian, Pouget-Abadie J] 中，每次学习参数的更新过程，被设为D更新k回，G才更新1回，也是出于类似的考虑。</li></ul><h5 id="GAN变种"><a href="#GAN变种" class="headerlink" title="GAN变种"></a>GAN变种</h5><p>GAN问世以后，得到了广泛研究，国外有大神整理了一个GAN zoo，里面是关于GAN的各种论文，有兴趣的可以看看：</p><p><a href="https://github.com/hindupuravinash/the-gan-zoo" target="_blank" rel="noopener">https://github.com/hindupuravinash/the-gan-zoo</a></p>]]></content>
      
      
      <categories>
          
          <category> 对抗生成网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习模型中的一些评估指标（四十五）</title>
      <link href="/2019/09/22/dl45/"/>
      <url>/2019/09/22/dl45/</url>
      
        <content type="html"><![CDATA[<p>对于不同的学习任务，比如分类问题，回归问题，我们学得的模型是不同的。对于不同的模型，选择合适的指标，从而判断它们的性能，进而改进这些模型，提高学习效率。这一次，就聊聊这些指标。</p><h4 id="分类问题（Classification）"><a href="#分类问题（Classification）" class="headerlink" title="分类问题（Classification）"></a>分类问题（Classification）</h4><h5 id="1-四个名词"><a href="#1-四个名词" class="headerlink" title="1.四个名词"></a>1.四个名词</h5><p>在介绍下面这些指标之前，我们先说4个基础的单词和它们缩写：</p><ul><li>True Positive（真正，TP）：将正类预测为正类数。</li><li>True Negative（真负，TN）：将负类预测为负类数。</li><li>False Positive（假正，FP）：将负类预测为正类数，误报（Type I error）。</li><li>False Negative（假负，FN）：将正类预测为负类数，漏报（Type II error）。</li></ul><h5 id="2-混淆矩阵"><a href="#2-混淆矩阵" class="headerlink" title="2. 混淆矩阵"></a>2. 混淆矩阵</h5><p>混淆矩阵是对分类结果进行详细描述的一张表，主要用于比较分类结果和样本的信息。对于二分类问题，混淆矩阵如下表所示：</p><p><img src="/2019/09/22/dl45/dl4501.jpg" alt="混淆矩阵"></p><h5 id="3-准确率（Accuracy）"><a href="#3-准确率（Accuracy）" class="headerlink" title="3. 准确率（Accuracy）"></a>3. 准确率（Accuracy）</h5><p>对于给定的测试数据集，分类器正确分类的样本总数（包含正负样本）与原始数据集总样本数之比。<br>$$<br>Accuracy = \frac {TP+TN} {TP+TN+FN+FP}<br>$$</p><blockquote><p>如果正负样本不均衡，不能只用准确率做为评价指标。</p></blockquote><h5 id="4-精准率-查准率（Precision）"><a href="#4-精准率-查准率（Precision）" class="headerlink" title="4. 精准率/查准率（Precision）"></a>4. 精准率/查准率（Precision）</h5><p>针对预测结果而言，预测为正类的样本（真正和假正）中，真的正类的比率。<br>$$<br>Precision = \frac {TP} {TP+FP}<br>$$</p><h5 id="5-召回率-查全率-（Recall）"><a href="#5-召回率-查全率-（Recall）" class="headerlink" title="5. 召回率/查全率  （Recall）"></a>5. 召回率/查全率  （Recall）</h5><p>针对样本而言，被正确判定的正实例（TP）在总的正实例中（真正和假负）的比率。<br>$$<br>Recall = \frac {TP} {TP+FN}<br>$$</p><h5 id="6-F-measure（F1-Score）"><a href="#6-F-measure（F1-Score）" class="headerlink" title="6. F-measure（F1 Score）"></a>6. F-measure（F1 Score）</h5><p>一般而言，召回率高时，精准率低；精准率高时，召回率低。虽然Precision和Recall没有必然关系，但是它们通常被一起用来评估模型。F-measure就是Precision和Recall的加权调和平均数。</p><p>F-measure的一般形式：<br>$$<br>F = \frac {(1+\beta^2) \times P \times R}{\beta^2 \times P +R}<br>$$<br>其中$\beta$表示查全率与查准率的权重，如果查全率的权重=查准率的权重，即查全率和查准率一样重要，那么上述公式就变成了：<br>$$<br>F1 = \frac{2 \times Recall \times Precision}{Recall+Precision}<br>$$<br>也就是我们常说的F1度量，F1 Score。</p><h5 id="7-PR曲线"><a href="#7-PR曲线" class="headerlink" title="7. PR曲线"></a>7. PR曲线</h5><p>以精准率Precision作为纵轴，以召回率Recall作为横轴所画的曲线，如下图所示：</p><p><img src="/2019/09/22/dl45/dl4502.JPG" alt="PR曲线"></p><p>PR曲线的具体绘制步骤是：</p><p>首先根据样本的置信度（confidence score）从大到小排序，这里置信度的意思是该样本是正样本的概率，比如90%的概率认为A样本是正例，10%的概率认为B样本是正例。</p><p>然后选取top-N（通常是top-5）的样本，预测为正例，剩下的样本预测为反例。</p><p>接着对每个样本计算Precision和Recall，就可以画出对应的曲线了。</p><h6 id="AP计算"><a href="#AP计算" class="headerlink" title="AP计算"></a>AP计算</h6><p>只是根据top-5来衡量一个模型的好坏，不太好，所以就有了另外一种叫做AP的计算方法，假设N个样本中有M个正例，那么我们会得到M个recall值（1/M, 2/M, … ,M/M），对于每个recall值r，我们可以计算出对应的（r’ &gt;= r）的最大precision，然后对这M个precision值取平均，即得到最后的AP值。比如下图所示：</p><p><img src="/2019/09/22/dl45/dl4503.JPG" alt="AP计算"></p><h5 id="8-受试者工作特征曲线（Receiver-Operating-Characteristic-curve，ROC）"><a href="#8-受试者工作特征曲线（Receiver-Operating-Characteristic-curve，ROC）" class="headerlink" title="8. 受试者工作特征曲线（Receiver Operating Characteristic curve，ROC）"></a>8. 受试者工作特征曲线（Receiver Operating Characteristic curve，ROC）</h5><p>以假正类率（False Positive Rate，FPR）为横轴，以真正类率（True Positive Rate，TPR）为纵轴所画的曲线。</p><p>假正类率：被模型判定为正类的负实例在实际的负样本中的比例。<br>$$<br>FPR = \frac{FP}{FP+TN}<br>$$<br>真正类率：被模型判定为正类的正实例在实际的正样本中的比例。<br>$$<br>TPR = \frac{TP}{TP+FN}<br>$$</p><p>ROC曲线上的每个点对应的是在某个阈值（threshold）下得到的（FPR，TPR）。设定一个阈值，大于这个阈值的实例被划分为正实例，小于这个值得实例则被划分为负实例，运行模型，得出结果，计算FPR和TPR的值。更换阈值，循环操作，就得到不同阈值下的（FPR，TPR），即可绘制成ROC曲线。并且ROC曲线在测试集中的正负样本的分布变化时，能够保持不变。</p><p>下图显示了一条ROC曲线：</p><p><img src="/2019/09/22/dl45/dl4504.png" alt="ROC曲线"></p><p>ROC曲线与FPR围成的面积（AUC）越大，说明性能越好。一般来说，如果ROC是光滑的，那么基本可以判断没有太大的overfitting。</p><h5 id="9-AUC（Area-Under-Curve）"><a href="#9-AUC（Area-Under-Curve）" class="headerlink" title="9. AUC（Area Under Curve）"></a>9. AUC（Area Under Curve）</h5><p>ROC曲线的面积，在0.5到1.0之间。之所以用AUC值做为评判标准，是因为很多时候不能从ROC曲线中判别模型的好坏。AUC的值越接近1，说明模型性能越好，模型预测的准确率越高。</p><h4 id="回归问题（Regression）"><a href="#回归问题（Regression）" class="headerlink" title="回归问题（Regression）"></a>回归问题（Regression）</h4><h5 id="1-平均绝对误差（Mean-Absolute-Error，MAE）"><a href="#1-平均绝对误差（Mean-Absolute-Error，MAE）" class="headerlink" title="1. 平均绝对误差（Mean Absolute Error，MAE）"></a>1. 平均绝对误差（Mean Absolute Error，MAE）</h5><p>绝对误差的平均值。</p><p>公式为：<br>$$<br>MAE = \frac{1}{m} \sum_{i=1}^m \left | h(x_i)-y_i \right |<br>$$</p><h5 id="2-均方误差（Mean-Square-Error，MSE）"><a href="#2-均方误差（Mean-Square-Error，MSE）" class="headerlink" title="2. 均方误差（Mean Square Error，MSE）"></a>2. 均方误差（Mean Square Error，MSE）</h5><p>真实值与预测值的差值的平方，然后求和平均。常被用作线性回归的损失函数。</p><p>公式为：<br>$$<br>MSE = \frac {1}{m} \sum_{i=1}^m (y_i - \hat y_i)^2<br>$$<br>其中$y_i$表示真实值，$\hat y_i$表示预测值。</p><h5 id="3-均方根误差-（Root-Mean-Square-Error，RMSE）"><a href="#3-均方根误差-（Root-Mean-Square-Error，RMSE）" class="headerlink" title="3. 均方根误差 （Root Mean Square Error，RMSE）"></a>3. 均方根误差 （Root Mean Square Error，RMSE）</h5><p>衡量观测值与真实值之间的偏差。</p><p>公式为：<br>$$<br>RMSE = \sqrt {\frac {1}{m} \sum_{i=1}^m (h(x_i) - y_i)^2}<br>$$</p><h5 id="4-决定系数（Coefficient-of-determination）"><a href="#4-决定系数（Coefficient-of-determination）" class="headerlink" title="4.决定系数（Coefficient of determination）"></a>4.决定系数（Coefficient of determination）</h5><p>也被称为$R^2$（R-Square）。</p><p>公式为：<br>$$<br>R2= 1 - \frac{\sum_i (\hat y_i-y_i)^2}{\sum_i (\bar y_i - y_i)^2}<br>$$<br>其中，分子部分表示真实值与预测值的平方差之和，类似于均方差MSE；分母部分表示真实值与均值的平方差之和，类似于方差Var。</p><p>R-square的取值范围为[0,1]：</p><p>如果结果是0，说明模型拟合效果很差；</p><p>如果结果是1，说明模型无错误。</p><p>一般来说，R-Square越大，表示模型拟合效果越好。</p>]]></content>
      
      
      <categories>
          
          <category> 模型评估 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 模型评估指标 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>这！就是深度学习之循环神经网络（四十四）</title>
      <link href="/2019/09/15/dl44/"/>
      <url>/2019/09/15/dl44/</url>
      
        <content type="html"><![CDATA[<p>循环神经网络（Recurrent Neural Network，RNN）是一类用于处理序列数据的神经网络。序列数据的特点就是前后之间有很强的关联性，也就是说前面出现的数据（比如词汇）对后面的数据有重大的影响，甚至后面的数据对前面的数据也是有重要影响的（双向循环神经网络）。例如句子，文档，语音都可以算是序列。</p><p>对于CNN来说，它的输出只考虑了前一个输入的影响而不考虑其它时刻输入的影响，对于猫，狗，手写数字等单个物体的识别具有较好的效果。但是，对于一些与时间先后有关的，比如视频的下一刻的预测，文档前后文内容的预测等等，这些算法的表现不尽人意，因此，RNN就应运而生了。</p><p>RNN不仅考虑前一刻的输入，而且对前面的内容有一种“记忆”功能。隐藏层之间的节点不再无连接而是有连接的，并且隐藏层的输入不仅包括输入层的输出，还包括上一时刻隐藏层的输出。</p><h5 id="RNN的结构"><a href="#RNN的结构" class="headerlink" title="RNN的结构"></a>RNN的结构</h5><p>一个简单的RNN单元如下，左边是折叠式，右边是展开式：</p><p><img src="/2019/09/15/dl44/dl4401.JPG" alt="RNN结构"></p><p>上图中，t-1，t，t+1表示时间序列，x表示输入的样本，$s_t$表示样本在t时刻的记忆，并且$s_t=f(W<em>S_{t-1}+U</em>X_t)$，W表示输入的权重，U表示此刻输入的样本的权重，V表示输出的样本权重。可以看到这里的权重W，U，V是共享的。</p><h5 id="RNN的前向传播"><a href="#RNN的前向传播" class="headerlink" title="RNN的前向传播"></a>RNN的前向传播</h5><p>在t=1时刻，一般初始化输入$s_0=0$，随机初始化W，U，V，进行下面的公式计算：<br>$$<br>h_1=Ux_1+Ws_0<br>$$</p><p>$$<br>s_1=f(h_1)<br>$$</p><p>$$<br>o_1=g(Vs_1)<br>$$</p><p>其中f和g均为激活函数，f可以是tanh，relu，sigmoid等激活函数，常用为tanh，g通常是softmax函数。</p><p>时间往前推进，此时的状态$s_1$做为时刻1的记忆状态将参加下一时刻的预测活动，也就是：<br>$$<br>h_2=Ux_2+Ws_1<br>$$</p><p>$$<br>s_2=f(h_2)<br>$$</p><p>$$<br>o_2=g(Vs_2)<br>$$</p><p>以此类推，可以得到最终的输出值为：<br>$$<br>h_t=Ux_t+Ws_{t-1}<br>$$</p><p>$$<br>s_t=f(h_t)<br>$$</p><p>$$<br>o_t=g(Vs_t)<br>$$</p><p>总结为一句话就是：新的输出是由新的输入和之前被记录下来的值所决定的。</p><h5 id="RNN的反向传播"><a href="#RNN的反向传播" class="headerlink" title="RNN的反向传播"></a>RNN的反向传播</h5><p>反向传播做的事情就是更新权重参数W，U，V，每一次的输出值$O_t$都会产生一个误差值$E_t$,则总的误差可以表示为：$E=\sum_{t}e_t$，损失函数可以使用交叉熵损失函数或者平方误差损失函数。</p><p>前面我们说过反向传播（Back Propagation ）算法，但是RNN除了依赖前一步的输出，还需要前若干步网络的状态，所以改进版的BP算法叫做Back Propagation Through Time（BPTT）算法。</p><p>这里我们选用激活函数tanh和softmax来做说明，并且输出用y表示之前的o，则：<br>$$<br>s_t=tanh(Ux_t + Ws_{t-1})<br>$$</p><p>$$<br>\hat y = softmax(Vs_t)<br>$$<br>我们定义损失函数为交叉熵损失，所以每一时刻的误差为：<br>$$<br>E_t(y_t,\hat y_t) = -y_t log \hat y_t<br>$$<br>总的误差为：<br>$$<br>E(y,\hat y)=\sum_t E_t(y_t,\hat y_t) = -\sum_t y_tlog \hat y_t<br>$$<br>这里的$y_t$是t时刻的正确单词，$\hat y_t$是网络的预测。比如，我们将完整的序列（句子）做为一个训练实例，总的误差则是各个时间点（单词）误差的和。</p><p>我们的目的是计算误差关于权重参数U，V和W的梯度，并通过梯度下降法（SGD）来更新权重参数。我们用$E_3$作为例子来推导。</p><p>首先看V的梯度：<br>$$<br>\frac{\partial E_3}{\partial V} = \frac{\partial E_3}{\partial \hat y_3} \frac{\partial \hat y_3}{\partial V} = \frac{\partial E_3}{\partial \hat y_3} \frac{\partial \hat y_3}{\partial z_3} \frac{\partial z_3}{\partial V} = (\hat y_3 - y_3) \bigotimes s_3<br>$$<br>上式中，我们定义$z_3 = Vs_3, \bigotimes$是两个向量的外积。因为已知$\hat y_3,y_3,s_3$,所以V的梯度可以直接计算出来。</p><p>接着看W的梯度：<br>$$<br>\frac{\partial E_3}{\partial W} = \frac{\partial E_3}{\partial \hat y_3} \frac{\partial \hat y_3}{\partial s_3} \frac{\partial s_3}{\partial W}<br>$$<br>其中$s_3=tanh(Ux_3+Ws_2)$,依赖于$s_2$.而$s_2$依赖于W和$s_1$，所以对于W的求导，需要用到链式法则，即：<br>$$<br>\frac{\partial E_3}{\partial W} = \sum_{k=0}^3 \frac{\partial E_3}{\partial \hat y_3} \frac{\partial \hat y_3}{\partial s_3} \frac{\partial s_3}{\partial s_k} \frac{\partial s_k}{\partial W}<br>$$<br>由于W在每时每刻都作用在我们所关心的输出上，所以我们需要从时刻t=3，通过网络的所有路径到时刻t=0来反向传播梯度，如下图所示：</p><p><img src="/2019/09/15/dl44/dl4402.JPG" alt="BPTT"></p><p>同理可以求得U的梯度：<br>$$<br>\frac{\partial E_3}{\partial U} = \sum_{k=0}^3 \frac{\partial E_3}{\partial \hat y_3} \frac{\partial \hat y_3}{\partial s_3} \frac{\partial s_3}{\partial x_k} \frac{\partial x_k}{\partial U}<br>$$</p><h5 id="长短时记忆网络（Long-Short-Term-Memory，LSTM）结构"><a href="#长短时记忆网络（Long-Short-Term-Memory，LSTM）结构" class="headerlink" title="长短时记忆网络（Long Short Term Memory，LSTM）结构"></a>长短时记忆网络（Long Short Term Memory，LSTM）结构</h5><p>当前预测位置和相关信息之间的文本间隔不断增大时，简单循环神经网络有可能丧失学习到距离如此远的信息的能力，或者在复杂语言场景中，有用信息的间隔有大有小，长短不一，循环神经网络的性能也会受到限制。</p><p>长短时记忆模型的设计就是为了解决这个问题。采用LSTM结构的循环神经网络比标准的循环神经网络表现更好。与单一tanh循环体结构不同，LSTM是一种拥有三个“门”结构的特殊网络结构。</p><h6 id="1-遗忘门"><a href="#1-遗忘门" class="headerlink" title="1. 遗忘门"></a>1. 遗忘门</h6><p>遗忘门的结构如下：</p><p><img src="/2019/09/15/dl44/dl4403.JPG" alt="遗忘门"></p><p>它决定丢掉细胞状态的哪些信息。根据$h_{t-1}$和$x_t$，遗忘门为状态$C_{t-1}$输出一个介于0到1之间的数字，0表示完全丢弃，1表示完全接受，数学表达式为：</p><p>$f_t=\sigma(W_f[h_{t-1},x_t]+b_f)$</p><h6 id="2-输入门"><a href="#2-输入门" class="headerlink" title="2.输入门"></a>2.输入门</h6><p>输入门的结构如下：</p><p><img src="/2019/09/15/dl44/dl4404.JPG" alt="输入门"></p><p>它由两部分构成，第一部分为sigmoid函数，输出为$i_t$，决定更新哪些值，第二部分为tanh激活函数，输出为$\tilde {C_t}$，$i_t$和$\tilde {C_t}$相乘后的结构用于更新细胞状态，数学表达式为：</p><p>$i_t = \sigma(W_i[h_{t-1} ,x_t]+b_i)$</p><p>$\tilde {C_t} =tanh(W_C[h_{t-1},x_t]+b_C)$</p><h6 id="3-输出门"><a href="#3-输出门" class="headerlink" title="3 输出门"></a>3 输出门</h6><p>经过遗忘门换个输入门，细胞状态更新为：</p><p><img src="/2019/09/15/dl44/dl4405.JPG" alt="输出门01"></p><p>输出基于上述细胞状态，但是需要过滤，输出门结构如下：</p><p><img src="/2019/09/15/dl44/dl4406.JPG" alt="输出门02"></p><p>首先，我们使用sigmoid层决定输出细胞状态的哪些部分。然后，我们令细胞状态通过tanh层，输出结果与sigmoid层的输出结果相乘。数学公式为:</p><p>$o_t = \sigma(W_o[h_{t-1,x_t}]+b_o)$</p><p>$h_t=o_t*tanh(C_t)$</p><h6 id="LSTM的前向传播算法"><a href="#LSTM的前向传播算法" class="headerlink" title="LSTM的前向传播算法"></a>LSTM的前向传播算法</h6><ol><li><p>更新遗忘门的输出：</p><p>$f_t=\sigma(W_f[h_{t-1},x_t]+b_f)$</p></li><li><p>更新输入门的输出：</p><p>$i_t = \sigma(W_i[h_{t-1},x_t]+b_i)$</p><p>$\tilde {C_t} =tanh(W_C[h_{t-1},x_t]+b_C)$</p></li><li><p>更新细胞状态：</p><p>$C_t = f_t<em>C_{t-1}+i_t</em> \tilde {C_t}$</p></li><li><p>更新输出门的输出：</p><p>$o_t = \sigma(W_o[h_{t-1,x_t}]+b_o)$</p><p>$h_t=o_t*tanh(C_t)$$</p></li></ol><h5 id="双向循环神经网络（BRNN）"><a href="#双向循环神经网络（BRNN）" class="headerlink" title="双向循环神经网络（BRNN）"></a>双向循环神经网络（BRNN）</h5><p>对于语言建模来说，很多时候只看前面的词是不够的的，比如下面这句话：</p><blockquote><p>我的手机坏了，我打算____一部新手机。</p></blockquote><p>如果只看横线前面的词，手机坏了，我是打算修一修？哭一哭？还是买一部新的呢？这些是无法确定的，但是根据横线后面的词语“一部新手机”，那么大概率，我会“买”。</p><p>这个时候就需要双向循环神经网络，如下图所示：</p><p><img src="/2019/09/15/dl44/dl4407.JPG" alt="双向循环神经网络"></p><h5 id="深度循环神经网络（DRNN）"><a href="#深度循环神经网络（DRNN）" class="headerlink" title="深度循环神经网络（DRNN）"></a>深度循环神经网络（DRNN）</h5><p>前面提到的RNN都是只有一层隐藏层，如果将多个RNN单元堆叠在一起，那就形成了深度循环神经网络，或者叫多层RNN。它的网络结构如下：</p><p><img src="/2019/09/15/dl44/dl4408.JPG" alt="深度循环神经网络"></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RNN </tag>
            
            <tag> LSTM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>这！就是概率图模型之隐马尔可夫模型和条件随机场（四十三）</title>
      <link href="/2019/09/15/dl43/"/>
      <url>/2019/09/15/dl43/</url>
      
        <content type="html"><![CDATA[<p>在说条件随机场和隐马尔可夫模型之前，先说两个别的概念：判别式模型和生成式模型。</p><h5 id="判别式模型（Discriminative-Model）"><a href="#判别式模型（Discriminative-Model）" class="headerlink" title="判别式模型（Discriminative Model）"></a>判别式模型（Discriminative Model）</h5><p>直接对条件概率$p(y|x;\theta)$建模，即在特征x出现的情况下标记y出现的概率。假设现在有一个分类问题，要根据动物的特征来区分大象（y=1）和狗（y=0），给定这样的一种数据集，回归模型比如逻辑回归，会师徒找到一条直线也就是决策边界，来区分大象和狗这两类，然后对于新来的样本，回归模型会根据这个新样本的特征去计算这个样本落在决策边界的哪一边，从而得到相应的分类结果。</p><p>常见的判别式模型有：</p><ul><li>线性回归 （Linear Regression）</li><li>逻辑斯蒂回归（Logistic Regression）</li><li>神经网络（NN）</li><li>支持向量机（SVM）</li><li>K近邻（KNN）</li><li>高斯过程（Gaussian Process）</li><li>条件随机场（Conditional Random Field，CRF）</li><li>CART（Classification and Regression Tree）</li><li>Boosting</li><li>线性判别分析法（LDA）</li></ul><h5 id="生成式模型（Generative-Model）"><a href="#生成式模型（Generative-Model）" class="headerlink" title="生成式模型（Generative Model）"></a>生成式模型（Generative Model）</h5><p>对x和y的联合分布$p(x,y)$建模，然后通过贝叶斯公式来求得$p(y_i|x)$，最后选取使得$p(y_i|x)$最大的$y_i$。比如还是想要区分大象和狗这个问题，首先根据训练集中的大象样本，建立大象的模型，根据训练集中狗的样本，再建立狗的模型。对于新来的样本，可以让它与大象模型匹配看概率有多少，再与狗模型匹配看概率有多少，哪一个概率大，就属于哪一类。</p><p>常见的生成式模型有：</p><ul><li>朴素贝叶斯 （Naive Bayes）</li><li>高斯混合模型（GMM）</li><li>隐马尔可夫模型（HMM）</li><li>贝叶斯网络 （Bayesian Networks）</li><li>Sigmoid Belief Networks</li><li>马尔可夫随机场（Markov Random Fields）</li><li>深度信念网络（DBN）</li><li>限制玻尔兹曼机</li></ul><p>由上面的说明可以看到隐马尔可夫模型属于生成式模型，条件随机场属于判别式模型，接着说说这两种模型是个啥。</p><h5 id="隐马尔可夫模型（Hidden-Markov-Model，HMM）"><a href="#隐马尔可夫模型（Hidden-Markov-Model，HMM）" class="headerlink" title="隐马尔可夫模型（Hidden Markov Model，HMM）"></a>隐马尔可夫模型（Hidden Markov Model，HMM）</h5><p>隐马尔可夫模型是关于时序的概率模型，描述由一个隐藏的马尔可夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测而产生观测随机序列的过程。</p><h6 id="马尔可夫链（Markov-Chain）"><a href="#马尔可夫链（Markov-Chain）" class="headerlink" title="马尔可夫链（Markov Chain）"></a>马尔可夫链（Markov Chain）</h6><p>也称为离散时间马尔可夫链，因俄国数学家安德烈·马尔可夫得名，描述为状态空间中从一个状态到另一个状态转换的随机过程。该过程要求具备”无记忆“的性质：下一状态$S_{t+1}$的概率分布只能由当前状态$S_t$决定，与之前的状态无关，即：<br>$$<br>P(S_{t+1}|S_1,S_2,…,S_t)=P(S_{t+1}|S_t)<br>$$<br>这种特定类型的“无记忆性”称作马尔可夫性质，符合该性质的随机过程称为马尔可夫过程，也称为马尔可夫链。</p><p>比如下图中有两种状态A和B，如果我们在A，接下来可以过渡到B或留在A。如果我们在B，可以过渡到A或者留在B。</p><p><img src="/2019/09/15/dl43/dl4301.JPG" alt="马尔可夫链示意图"></p><h6 id="隐马尔可夫模型的三要素"><a href="#隐马尔可夫模型的三要素" class="headerlink" title="隐马尔可夫模型的三要素"></a>隐马尔可夫模型的三要素</h6><p>隐马尔可夫模型由初始状态概率向量π，状态转移概率矩阵A和观测概率矩阵B决定。π和A决定状态序列，B决定观测序列。因此，隐马尔可夫模型λ可以用三元符号表示，即：<br>$$<br>\lambda = (A,B,\pi)<br>$$<br>A,B,π称为隐马尔可夫模型的三要素。</p><p>举个例子，假设有4个盒子，每个盒子里都装有红白两种颜色的球，盒子里的红白球数如下表所示。</p><table><thead><tr><th align="center">盒子</th><th align="center">1</th><th align="center">2</th><th align="center">3</th><th align="center">4</th></tr></thead><tbody><tr><td align="center">红球数</td><td align="center">5</td><td align="center">3</td><td align="center">6</td><td align="center">8</td></tr><tr><td align="center">白球数</td><td align="center">5</td><td align="center">7</td><td align="center">4</td><td align="center">2</td></tr></tbody></table><p>按照下面的方法抽球，产生一个球的颜色的观测序列：开始，从4个盒子里以等概率随机选取1个盒子，从这个盒子里随机抽出1个球，记录其颜色后，放回。然后，从当前盒子随机转移到下一个盒子，规则是：如果当前盒子是盒子1，那么下一个盒子一定是盒子2，如果当前是盒子2或者3，那么分别以概率0.4和0.6转移到左边或右边的盒子，如果当前是盒子4，那么各以0.5的概率停留在盒子4或转移到盒子3，确定转移的盒子后，再从这个盒子里随机抽出1个球，记录其颜色，放回。如此下去，重复进行5次，得到一个球的颜色的观测序列为：<br>$$<br>O=\{红，红，白，白，红\}<br>$$<br>在这个过程中，观察者只能观测到球的颜色的序列，观测不到球是从哪个盒子取出的，即观测不到盒子的序列。</p><p>在这个例子中有两个随机序列，一个是盒子的序列（状态序列），一个是球的颜色的观测序列（观测序列）。前者是隐藏的，只有后者是可观测的。根据条件，可以明确状态集合，观测集合，序列长度以及模型的三要素。</p><p>盒子对应状态，状态的集合是：<br>$$<br>Q=\{盒子1，盒子2，盒子3，盒子4\}， N=4<br>$$<br>球的颜色对应观测，观测的集合是：<br>$$<br>V = \{红，白\}， M=2<br>$$<br>状态序列和观测序列的长度$T=5$。</p><p>初始概率分布为：<br>$$<br>\pi=(0.25,0.25,0.25,0.25)^T<br>$$<br>状态转移概率分布为：<br>$$<br>A=\begin{bmatrix}<br> 0 &amp; 1 &amp; 0 &amp; 0\\<br> 0.4 &amp; 0 &amp; 0.6 &amp; 0\\<br> 0 &amp; 0.4 &amp; 0 &amp; 0.6\\<br> 0 &amp; 0 &amp; 0.5 &amp; 0.5<br>\end{bmatrix}<br>$$<br>观测概率分布为：<br>$$<br>B=\begin{bmatrix}<br> 0.5 &amp; 0.5\\<br> 0.3 &amp; 0.7\\<br> 0.6 &amp; 0.4\\<br> 0.8 &amp; 0.2<br>\end{bmatrix}<br>$$</p><h6 id="隐马尔可夫模型的两个基本假设"><a href="#隐马尔可夫模型的两个基本假设" class="headerlink" title="隐马尔可夫模型的两个基本假设"></a>隐马尔可夫模型的两个基本假设</h6><ol><li>齐次马尔可夫性假设，即假设隐藏的马尔可夫链在任意时刻t的状态只依赖于其前一时刻的状态，于其他时刻的状态及观测无关，也与时刻t无关。</li></ol><p>$$<br>P(i_t|i_{t-1},o_{t-1},\cdots,i_1,o_1)=P(i_t|i_{t-1}), t=1,2,\cdots,T<br>$$</p><ol start="2"><li>观测独立性假设，即假设任意时刻的观测只依赖于该时刻的马尔可夫链的状态，与其他观测及状态无关。    </li></ol><p>$$<br>P(o_t|i_T,o_T,i_{T-1},o_{T-1},\cdots,i_{t+1},o_{t+1},i_t,i_{t-1},o_{t-1},\cdots,i_1,o_1)=P(o_t|i_t)<br>$$</p><h6 id="隐马尔可夫模型的三个基本问题"><a href="#隐马尔可夫模型的三个基本问题" class="headerlink" title="隐马尔可夫模型的三个基本问题"></a>隐马尔可夫模型的三个基本问题</h6><ol><li>概率计算问题。给定模型$\lambda=(A,B,\pi)$和观测序列$O=(o_1,o_2,\cdots,o_T)$，计算在模型λ下观测序列O出现的概率$P(O|\lambda)$。</li><li>学习问题。已知观测序列$O=(o_1,o_2,\cdots,o_T)$，估计模型$\lambda=(A,B,\pi)$参数，使得在该模型下观测序列概率$P(O|\lambda)$最大。即用极大似然估计的方法估计参数。</li><li>预测问题。也称为解码（decoding）问题。已知模型$\lambda=(A,B,\pi)$和观测序列$O=(o_1,o_2,\cdots,o_T)$，求对给定观测序列条件概率$P(I|O)$最大的状态序列$I=(i_1,i_2,\cdots,i_T)$。即给定观测序列，求最有可能的对应的状态序列。</li></ol><h6 id="隐马尔可夫模型的三个基本问题的解法"><a href="#隐马尔可夫模型的三个基本问题的解法" class="headerlink" title="隐马尔可夫模型的三个基本问题的解法"></a>隐马尔可夫模型的三个基本问题的解法</h6><ol><li>概率计算问题。可以采用直接计算法，前向-后向（forward-backward algorithm）算法。</li><li>学习问题。可以采用监督学习方法，鲍姆-韦尔奇（Baum-Welch）算法。</li><li>预测问题。可以采用近似算法，维特比（Viterbi）算法。</li></ol><p>具体的算法我就不细说了，如果以后用到了再详细梳理一下。</p><h5 id="条件随机场（Conditional-Random-Field，CRF）"><a href="#条件随机场（Conditional-Random-Field，CRF）" class="headerlink" title="条件随机场（Conditional Random Field，CRF）"></a>条件随机场（Conditional Random Field，CRF）</h5><p>条件随机场是给定随机变量X条件下，随机变量Y的马尔可夫随机场。</p><h6 id="马尔科夫随机场（Markov-Random-Field）"><a href="#马尔科夫随机场（Markov-Random-Field）" class="headerlink" title="马尔科夫随机场（Markov Random Field）"></a>马尔科夫随机场（Markov Random Field）</h6><p>概率图模型（Probabilistic Graphical Model，PGM）是由图表示的概率分布。概率无向图模型（Probabilistic Undirected Graphical Model）又称为马尔可夫随机场，表示一个联合概率分布，其标准定义为：设有联合概率分布P（V）由无向图G=（V，E）表示，图G中的节点表示随机变量，边表示随机变量的依赖关系。如果联合概率分布P（V）满足成对，局部或全局马尔可夫性，就称此联合概率分布为概率无向图模型或马尔可夫随机场。</p><ol><li><p>成对马尔可夫性</p><p>设无向图G中的任意两个没有边连接的节点u，v，其它所有节点为O，成对马尔可夫性指给定$Y_O$的条件下，$Y_u$和$Y_v$条件独立：$P(Y_u,Y_v|Y_O)=P(Y_u|Y_O)P(Y_v|Y_O)$。</p><p><img src="/2019/09/15/dl43/dl4302.JPG" alt="成对马尔可夫性"></p></li></ol><ol start="2"><li><p>局部马尔可夫性</p><p>设无向图G的任一节点v，W是与v有边相连的所有节点，O是v，W外的其它所有节点，局部马尔可夫性指在给定$Y_W$的条件下，$Y_v$和$Y_O$条件独立：$P(Y_v,Y_O|Y_W)=P(Y_v|Y_W)P(Y_O|Y_W)$。</p><p>当$P(Y_O|Y_W)&gt;0$时，等价于$P(Y_v|Y_W)=P(Y_v|Y_W,Y_O)$。</p><p><img src="/2019/09/15/dl43/dl4303.JPG" alt="局部马尔可夫性"></p></li></ol><ol start="3"><li><p>全局马尔可夫性</p><p>设节点集合A，B是在无向图G中被节点结合G分开的任意节点集合，全局马尔可夫性指给定$Y_C$的条件下，$Y_A$和$Y_B$条件独立，即：$P(Y_A,Y_B|Y_C)=P(Y_A|Y_C)P(Y_B|Y_C)$。</p><p><img src="/2019/09/15/dl43/dl4304.JPG" alt="全局马尔可夫性"></p></li></ol><h6 id="线性条件随机场（linear-chain-conditional-random-field）"><a href="#线性条件随机场（linear-chain-conditional-random-field）" class="headerlink" title="线性条件随机场（linear chain conditional random field）"></a>线性条件随机场（linear chain conditional random field）</h6><p>这里主要介绍定义在线性链上的特殊的条件随机场，称为线性条件随机场（linear chain conditional random field）。</p><p>设$X=(X_1,X_2,\cdots,X_n),Y=(Y_1,Y_2,cdots,Y_n)$均为线性链表示的随机变量序列，若在给定随机变量序列X的条件下，随机变量序列Y的条件概率分布P（X|Y）构成条件随机场，即满足马尔可夫性：<br>$$<br>P(Y_i|X,Y_1,\cdots,Y_{i-1},Y{i+1},\cdots,Y_n)=P(Y_i|X,Y_{i-1},Y_{i+1})<br>$$</p><p>$$<br>i=1,2,\cdots,n(在i=1和n时只考虑单边)<br>$$</p><p>则称P（X|Y）为线性条件随机场。在标注问题中，X表示输入观测序列，Y表示对应的输出标记序列或状态序列。</p><p><img src="/2019/09/15/dl43/dl4305.JPG" alt="线性条件随机场"></p><h6 id="条件随机场的概率计算问题"><a href="#条件随机场的概率计算问题" class="headerlink" title="条件随机场的概率计算问题"></a>条件随机场的概率计算问题</h6><p>条件随机场的概率计算问题是给定条件随机场P（X|Y），输入序列x和输出序列y，计算条件概率$P(Y_i=y_i|x),P(Y_{i-1}=y_{i-1},Y_i=y_i|x)$以及相应的数学期望问题。</p><ol><li><p>前向-后向算法</p><p>为了计算每个节点的概率，比如$P(Y=y_i|x)$的概率，对于这类概率，用前向或者后向算法其中的任何一个就可以解决。前向或后向算法，都是扫描一遍整体的边权值，计算P（X），只是它们扫描的方向不同，一个从后往前，一个从前往后。所以<br>$$<br>P(x)=Z(x)=\sum_yP(y,x)=\alpha_n^T(x)\cdot1=1^T \cdot \beta_1(x)<br>$$<br>上式中$\alpha$为前向向量，$\beta$为后向向量。</p><p>根据前向-后向向量的定义，很容易计算标记序列在位置$i$是标记$y_i$的条件概率和在位置$i-1$与$i$是标记$y_{i-1}$和$y_i$的条件概率：<br>$$<br>P(Y_i=y_i|x)=\frac{\alpha_i^T(y_i|x) \beta_i(y_i|x)}{Z(x)}<br>$$</p><p>$$<br>P(Y_{i-1}=y_{i-1},Y_i=y_i|x)=\frac{\alpha_{i-1}^T(y_{i-1}|x) M_i(y_{i-1},y_i|x) \beta_i(y_i|x)}{Z(x)}<br>$$</p><p>其中$Z(x)=\alpha_n^T(x) \cdot 1$</p></li></ol><ol start="2"><li><p>计算期望</p><p>利用前向-后向向量，可以计算特征函数关于联合分布P(X,Y)和条件分布P（Y|X）的数学期望。</p><p>特征函数$f_k$关于条件分布P（Y|X）的数学期望是：<br>$$<br>E_{P(Y|X)}[f_k]=\sum_y P(y|x)f_k(y,x)<br>$$</p><p>$$<br>=\sum_{i=1}^{n+1} \sum_{y_{i-1}y_i}f_k(y_{i-1},y_i,x,i) \frac{\alpha_{i-1}^T(y_{i-1}|x) M_i(y_{i-1},y_i|x) \beta_i(y_i|x)}{Z(x)},k=1,2,\cdots,K<br>$$</p><p>其中$Z(x)=\alpha_n^T(x) \cdot 1$</p></li></ol><p>​       假设经验分布为$\tilde{P}(X)$，特征函数$f_k$关于联合分布的数学期望是：<br>$$<br>E_{P(Y|X)}[f_k]=\sum_{x,y} P(x,y) \sum_{i=1}^{n+1} f_k(y_{i-1},y_i,x,i)<br>$$</p><p>$$<br>=\sum_x \tilde{P}(x) \sum_y P(y|x)\sum_{i=1}^{n+1} f_k(y_{i-1},y_i,x,i)<br>$$</p><p>$$<br>=\sum_x \tilde{P}(x) \sum_{i=1}^{n+1} \sum_{y_{i-1}y_i} f_k(y_{i-1},y_i,x,i) \frac{\alpha_{i-1}^T(y_{i-1}|x) M_i(y_{i-1},y_i|x) \beta_i(y_i|x)}{Z(x)},k=1,2,\cdots,K<br>$$</p><p>​        其中$Z(x)=\alpha_n^T(x) \cdot 1$</p><ol start="3"><li><p>参数学习算法</p><p>条件随机场模型实际上是定义在时序数据上的对数线性模型，其学习方法包括极大似然估计和正则化的极大似然估计。具体的优化算法有改进的迭代尺度法IIS，梯度下降法以及拟牛顿法。</p><p>假设经验分布为$\tilde{P}(X，Y)$，则训练数据的对数似然函数为：<br>$$<br>L(w)=L_{\tilde{p}} (P_w) = log \prod_{x,y}P_w(y|x)^{\tilde{p}(x,y)}=\sum_{x,y} \tilde{p}(x,y)logP_w(y|x)<br>$$</p></li><li><p>预测算法</p><p>采用动态规划思想的维特比算法。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 概率图模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HMM </tag>
            
            <tag> CRF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>这！就是深度学习之卷积神经网络（四十二）</title>
      <link href="/2019/09/08/dl42/"/>
      <url>/2019/09/08/dl42/</url>
      
        <content type="html"><![CDATA[<p>卷积神经网络（Convolutional  Neural Network，CNN）是一种专门用来处理具有类似网格结构的数据的神经网络，例如时间序列数据（可以认为是在时间轴上有规律地采样形成的一维网格）和图像数据（可以看作二维的像素网格）。</p><p>这里引用cs231n课程里的一副CNN的图片：</p><p><img src="/2019/09/08/dl42/dl4201.jpg" alt="卷积神经网络"></p><p>上面图片的意思是给定一张图片，利用卷积神经网络来判断它是汽车，卡车，飞机，轮船还是马。之前说过神经网络有输入层，隐藏层，输出层，卷积神经网络则把隐藏层做了更细致的划分，并且加深了神经网络的深度，即层数。根据上图，可以看到它包括CONV（卷积层），RELU（激励层），POOL（池化层），FC（全连接层），我们接下来看看这些层到底是干啥用的。</p><h4 id="CONV（卷积层）"><a href="#CONV（卷积层）" class="headerlink" title="CONV（卷积层）"></a>CONV（卷积层）</h4><p>这也是卷积神经网络中卷积的来源。卷积是一种运算，它是输入矩阵某一块和卷积核（或者叫滤波器，filter）的点积。这里引用吴恩达老师深度学习中的一张图：</p><p><img src="/2019/09/08/dl42/dl4202.png" alt="卷积运算"></p><p>一个6*6的矩阵，和一个3*3的filter做卷积运算，得到一个实数，填充到一个4*4的输出矩阵的左上角，这个绿色框框中的值就是：<br>$$<br>3 \times 1 + 0 \times 0 + 1 \times -1 + 1 \times 1 + 5 \times 0 + 8 \times -1 + 2 \times 1 + 7 \times 0 + 2 \times -1 = -5<br>$$<br>然后这个filter接着往右移动，一次移动1格，移到头了，接着再从最左边往下移动1格，然后再往右移动，简单来说，就是从左往右，从上到下。填充完毕的结果如下图：</p><p><img src="/2019/09/08/dl42/dl4203.png" alt="卷积运算结果"></p><p><strong>这个时候，你可能会有一个疑问，4*4的矩阵是怎么来的？</strong></p><p>这里有一个公式：<br>$$<br>out = \frac {input - filter} {stride} +1<br>$$<br>上面的式子中，input表示输入层的大小，filter表示卷积核的大小，stride表示步长，就是一次移动多少步，对于上面的例子有：<br>$$<br>out = \frac {6-3} {1} + 1 = 4<br>$$<br>所以，我们得到了4*4的输出矩阵。</p><p><strong>这个时候，你可能还有个疑问，如果$\frac {input - filter} {stride}$不是整数怎么办？</strong></p><p>我们一般会上取整，也就是不管1.XXX…，我们只取1。</p><p>当然还有一种更常见的做法是在输入矩阵的周围加上padding，如下图所示：</p><p><img src="/2019/09/08/dl42/dl4204.png" alt="Padding"></p><p>padding的值可以全为0，也可以和周边元素的值相同。这个时候，输出矩阵的大小就变成了：<br>$$<br>out = \frac {input - filter +2 \times padding} {stride} +1<br>$$</p><h4 id="RELU（激励层）"><a href="#RELU（激励层）" class="headerlink" title="RELU（激励层）"></a>RELU（激励层）</h4><p>激励层就是用激活函数ReLU作用在输出矩阵的输出，你如果还记得ReLU函数：</p><p><img src="/2019/09/08/dl42/dl4205.jpg" alt="激活函数ReLU"></p><p>可以看到当自变量&lt;0, 因变量=0，所以它的收敛速度比较快，求梯度的速度也比较快。</p><h4 id="POOL（池化层）"><a href="#POOL（池化层）" class="headerlink" title="POOL（池化层）"></a>POOL（池化层）</h4><p>池化层也可以看作是输入矩阵和滤波器的一种运算，只不过这种运算不是点积，而是取当前窗口矩阵中所有元素的最大值或者平均值，如下图所示：</p><p><img src="/2019/09/08/dl42/dl4206.jpg" alt="Pool"></p><p>因为只是取了一个值，所以它可以减少数据的空间尺寸。池化层之所有有效，是因为图片特征的不变性，也就是通过下采样不会丢失图片所拥有的特征，由于这种特性，我们可以将图片缩小再进行卷积处理，从而大大降低卷积运算的时间。</p><h4 id="FC（全连接层）"><a href="#FC（全连接层）" class="headerlink" title="FC（全连接层）"></a>FC（全连接层）</h4><p>全连接层和前面说过的神经网络模型是一样的，每个神经元与前一层所有的神经元全部链接。这个过程中为了防止过拟合会引入dropout，即随机丢掉一些神经元之间的连接。</p><h4 id="经典的神经网络结构"><a href="#经典的神经网络结构" class="headerlink" title="经典的神经网络结构"></a>经典的神经网络结构</h4><h5 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h5><p>CNN之父Lecun在1988年发明，用来识别手写数字。先来看看LeNet-5的网络结构：</p><p><img src="/2019/09/08/dl42/dl4207.png" alt="LeNet-5"></p><p>LeNet-5是针对灰度图片训练的，所有输入只有一个通道，假设你想识别上面的字母A，实际的图片大小就是$32 \times 32 \times 1$，使用6个$5 \times 5$，并且步幅为1的过滤器，那么输出就是$28 \times 28 \times 6$（其中$28=\frac{32-5}{1}+1$）,图像尺寸就从$32 \times 32$缩小到了$28 \times 28$。</p><p>然后进行池化操作，平均池化，过滤器的宽度为2，步幅为2，宽度和高度都缩小了2倍，输出结果是一个</p><p>$14 \times 14 \times 6$（其中$14=\frac{28-2}{2}+1$）的图像。</p><p>接下来使用16个$5 \times 5$，并且步幅为1的过滤器，新的输出结果有16个通道，输出图像大小为$10 \times 10 \times 16$（其中$10=\frac{14-5}{1}+1$）。</p><p>继续进行池化操作，宽高又缩小2倍，输出一个$5 \times 5 \times 16$的图像。将所有的数字相乘，得到400个节点。</p><p>下一层是全连接层，有400个节点，每一个节点有120个神经元。但有时还会从这400个节点中抽取一部分构建另外一个全连接层，如上图的C5和F6。</p><p>最后就是利用这84个特征得到最后的输出。我们可以再加一个节点用来预测$\hat y$的值，$\hat y$有10个可能的值，用来识别对应的输出。</p><p>从上图可以看出，随着网络越来越深，图像在变小，但是通道数在增加。</p><h5 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h5><p>它是Alex Krizhevsky发明的，另外两位合著者是IIya Sutskever和Geoffery Hinton, 并且在2012的ImageNet拿了冠军，它的网络结构如下：</p><p><img src="/2019/09/08/dl42/dl4208.png" alt="AlexNet"></p><p>AlexNet首先使用一张$227 \times 227 \times 3$的图片作为输入，这里的乘以3是因为彩色图片可以用R，G，B三原色来表示，所以有3个输入通道。</p><p>第一层使用96组$11 \times 11$的过滤器，步幅为4，图片输出为$55 \times 55$（其中$55=\frac{227-11}{4}+1$），缩小了4倍左右。</p><p>第二层使用一个$3 \times 3$的过滤器构建最大池化层，步幅s=2，卷积层图像缩小为$27 \times 27 \times 96$（其中$27=\frac{55-3}{2}+1$）</p><p>接着执行一个$5 \times 5$的卷积，padding之后，输出为$27 \times 27 \times 256$（其中$27=\frac{27-5+1\times2}{1}+1$）,图像大小没有变，但是通道数增加了不少。</p><p>接着进行最大池化，过滤器为$3 \times 3$，步幅为2，图像缩小为到$13 \times 13 \times 256$（其中$13=\frac{27-2}{2}+1$）。</p><p>再进行一次$3 \times 3$的same卷积，经过padding之后，输出为$13 \times 13 \times 384$。</p><p>再做一次same卷积，输出为$13 \times 13 \times 384$。</p><p>再做一次same卷积，输出为$13 \times 13 \times 256$。</p><p>接着做一次最大池化操作，过滤器为$3 \times 3 $，步幅为2，得到输出为$6 \times 6 \times 256$（其中$6=\frac{13-3}{2}+1$）。</p><p>将其展开为9216（$6 \times 6 \times 256$）个单元，然后就是全连接层，dropout一些连接，得到4096个单元，再dropout一些连接，最后得到1000个输出单元。</p><p>最后使用softmax函数输出识别的结果，看它究竟是1000个可能对象中的哪一个。</p><p>AlexNet比LeNet表现更出色的另一个原因是它使用了ReLU激活函数。</p><h5 id="VGG-Network"><a href="#VGG-Network" class="headerlink" title="VGG Network"></a>VGG Network</h5><p>它是牛津大学VGG实验室设计的架构，将AlexNet的8层提高到了19层，真正的体现了深度这个词的含义，这里我们介绍一下VGG-16，顾名思义，它有16层，它的网络结构如下：</p><p><img src="/2019/09/08/dl42/dl4209.png" alt="VGG Network"></p><p>比如要识别一个$224 \times 224 \times 3$的图像，在最开始的两层使用64个$3\times3$的过滤器，输出结果为$224 \times 224 \times 64$，接着创建一个池化层，将输入图像进行压缩，从$224 \times 224 \times 64$减少到$112 \times 112 \times 64$。然后又是若干个卷积层，使用129个滤波器，以及一些same卷积，输出图像大小为$112 \times 112 \times 128$，接着进行池化，最后输出图像为$56 \times 56 \times 128$，以此类推，最后将得到的$7 \times 7 \times 512$的特征图进行全连接，得到4096个单元，最后进行softmax激活，输出从1000个对象中识别的结果，过程图如下：</p><p><img src="/2019/09/08/dl42/dl4210.png" alt="VGG-16"></p><p>VGG大量的使用$3 \times 3$ 的卷积层，这一特点大大的简化了神经网络结构。</p><h5 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h5><p>它是谷歌实验室设计的架构，它的结构如下：</p><p><img src="/2019/09/08/dl42/dl4211.jpg" alt="GoogleNet"></p><p>它把网络的深度扩展为22层，大量的使用了Inception，并且经历了4个版本的inception。</p><h6 id="Inception-v1"><a href="#Inception-v1" class="headerlink" title="Inception v1"></a>Inception v1</h6><p>老版本的Inception v1结构如下：</p><p><img src="/2019/09/08/dl42/dl4212.png" alt="Old Inception v1"></p><p>如上图所示，初始版本的Inception v1，通过设计一个稀疏网络结构，从而产生稠密的数据。该结构将CNN中常用的卷积（1x1，3x3，5x5）和池化操作（3x3）堆叠在一起，一方面增加了网络的width，另一方面增加了网络对尺度的适应性。</p><p>该版本中所有的卷积核都在上一层的输出中进行，而5x5的卷积核需要的计算量比较到，造成了特征图的厚度很大，为了避免这种情况，在3x3和5x5前，max pooling之后分别加上了1x1的卷积核，从而降低特征图厚度，新的Inception v1结构如下：</p><p><img src="/2019/09/08/dl42/dl4213.jpg" alt="New Inception v1"></p><h6 id="Inception-v2"><a href="#Inception-v2" class="headerlink" title="Inception v2"></a>Inception v2</h6><p>大尺寸的卷积核可以带来更大的感受野，但也会产生更多的参数，比如5x5卷积核的参数为25个，但是3x3卷积核的参数只有9个，所以GooLeNet团队提出用2个连续的3x3卷积层组成小网络来代替单个的5x5卷积层。</p><p>通过大量的实验表明，这种分解不会造成表达缺失。那么还能不能分解的更小一点呢，答案是可以的，任意的nxn的卷积都可以通过1xn卷积后接nx1卷积来代替，如下图所示：</p><p><img src="/2019/09/08/dl42/dl4214.jpg" alt="Inception v2"></p><h6 id="Inception-v3"><a href="#Inception-v3" class="headerlink" title="Inception v3"></a>Inception v3</h6><p>v3一个最重要的改进是分解（Factorization），将7x7分解成两个一维的卷积（1x7,7x1），3x3也是一样（1x3,3x1），这样的好处，既可以加速计算（多余的计算能力可以用来加深网络），又可以将1个conv拆成2个conv，使得网络深度进一步增加，增加了网络的非线性，还有值得注意的地方是网络输入从224x224变为了299x299，更加精细设计了35x35/17x17/8x8的模块。</p><h6 id="Inception-v4"><a href="#Inception-v4" class="headerlink" title="Inception v4"></a>Inception v4</h6><p>随着神经网络的深度加深，梯度消失的可能性也越大。 Inception v4研究了Inception模块与残差连接（Residual Connection）的结合。</p><p>该结构如下图所示：</p><p><img src="/2019/09/08/dl42/dl4215.jpg" alt="Inception v4"></p><p>通过20个类似的模块组合，从而大大的加深了网络深度，还极大的提升了训练速度，同时性能也有提升。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 卷积神经网络 </tag>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>这！就是集成学习（四十一）</title>
      <link href="/2019/09/01/dl41/"/>
      <url>/2019/09/01/dl41/</url>
      
        <content type="html"><![CDATA[<p>集成学习（Ensemble Learning）就是用多个学习器合成一个强大的学习器来训练模型，从而提高模型的泛化能力。如果你听说过HA，它和HA有异曲同工之妙，本来一个server可以提供的服务，变成多个server来提供，提高了某种能力。</p><h4 id="Bagging（Bootstrap-Aggregation）"><a href="#Bagging（Bootstrap-Aggregation）" class="headerlink" title="Bagging（Bootstrap Aggregation）"></a>Bagging（Bootstrap Aggregation）</h4><p><img src="/2019/09/01/dl41/dl4101.jpg" alt="Bagging"></p><h5 id="过程："><a href="#过程：" class="headerlink" title="过程："></a>过程：</h5><ol><li>每一次都从原始样本集中抽取n个训练样本，然后放回，共进行k轮抽取，得到k个训练集。</li><li>每次使用一个训练集训练模型，共得到k个模型。</li><li>对于最终的结果，如果是分类问题，就采用投票的方法，如果是回归问题，就采用平均值的方法。</li><li>用于减少方差。</li></ol><h5 id="算法："><a href="#算法：" class="headerlink" title="算法："></a>算法：</h5><h6 id="随机森林（Random-Forest）"><a href="#随机森林（Random-Forest）" class="headerlink" title="随机森林（Random Forest）"></a>随机森林（Random Forest）</h6><p>使用CART决策树做为弱学习器的Bagging方法称为随机森林。特征随机性，采集和训练集样本数N一样个数的样本。</p><h4 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h4><p><img src="/2019/09/01/dl41/dl4102.jpg" alt="Boosting"></p><h5 id="过程：-1"><a href="#过程：-1" class="headerlink" title="过程："></a>过程：</h5><ol><li>先从初始训练集训练一个弱学习器。</li><li>根据学习误差率更新训练样本的权重，使得误差率高的样本在训练集中得到更多的重视，再训练一个新的弱学习器。</li><li>重复#2，直到弱学习器达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。</li><li>对于最终的结果，如果是分类任务，按照权重进行投票，如果是回归任务，进行加权，再进行预测。</li><li>用于减少偏差。</li></ol><h5 id="算法：-1"><a href="#算法：-1" class="headerlink" title="算法："></a>算法：</h5><h6 id="AdaBoost（Adaptive-boosting）"><a href="#AdaBoost（Adaptive-boosting）" class="headerlink" title="AdaBoost（Adaptive boosting）"></a>AdaBoost（Adaptive boosting）</h6><p>通过迭代每次学习一个基本分类器，每次迭代过程中，使用全部的样本，改变样本的权重，提高那些被前一轮弱分类器错误分类样本的权值，降低那些被正确分类样本的权值。最后，将基本分类器线性组合做为强分类器，其中给分类误差率小的弱分类器以大的权值，给分类错误率大的弱分类器以小的权值。</p><h6 id="Boosting-Tree"><a href="#Boosting-Tree" class="headerlink" title="Boosting Tree"></a>Boosting Tree</h6><p>提升树模型采用加法模型与前向分步算法，同时基函数采用决策树算法，对待分类问题采用二叉分类树，对于回归问题采用二叉回归树。提升树模型可以看做是决策树的加法模型。</p><h6 id="GBDT-（Gradient-Boost-Decision-Tree）"><a href="#GBDT-（Gradient-Boost-Decision-Tree）" class="headerlink" title="GBDT （Gradient Boost Decision Tree）"></a>GBDT （Gradient Boost Decision Tree）</h6><p>一方面可以从残差的角度来理解，每一颗回归树都是学习之前的树的残差（使用代价函数对上一轮训练出的模型函数f的偏导来拟合残差）。另一方面可以从梯度的角度掌握算法，即每一棵回归树通过梯度下降法学习之前树的梯度下降值。</p><h6 id="XGBoost-（eXtreme-Gradient-Boosting）"><a href="#XGBoost-（eXtreme-Gradient-Boosting）" class="headerlink" title="XGBoost （eXtreme Gradient Boosting）"></a>XGBoost （eXtreme Gradient Boosting）</h6><p>以分类回归树（CART树）进行组合。在XGBoost里，每棵树是一个一个往里添加的，每加一颗都是希望效果能够提升。为了限制叶子节点的个数，会在目标函数里加上一个惩罚项。</p><h4 id="Stacking"><a href="#Stacking" class="headerlink" title="Stacking"></a>Stacking</h4><p><img src="/2019/09/01/dl41/dl4103.jpg" alt="Stacking"></p><p>学习几个不同的弱学习器，并通过一个元模型来组合它们，然后基于弱模型返回的多个预测结果输出最终的预测结果。</p><h5 id="过程：-2"><a href="#过程：-2" class="headerlink" title="过程："></a>过程：</h5><ol><li>使用k折交叉验证法，弱学习器在k-1折数据上进行训练，并用剩下的1折进行预测。</li><li>使用弱学习器的预测结果做为元模型的输入，从而预测出最终的预测结果。</li></ol><p>集成学习也是一块蛮大的东西，这里只是开个头，没有细究算法的具体实现，毕竟只是听着名字，不知道意思，也不好。</p>]]></content>
      
      
      <categories>
          
          <category> 集成学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Bagging </tag>
            
            <tag> Boosting </tag>
            
            <tag> Stacking </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一些排序算法</title>
      <link href="/2019/08/25/sort/"/>
      <url>/2019/08/25/sort/</url>
      
        <content type="html"><![CDATA[<p>这里介绍几种排序算法，主要是刷LeetCode上的题。</p><h5 id="1-双指针"><a href="#1-双指针" class="headerlink" title="1 双指针"></a>1 双指针</h5><ol start="167"><li>两数之和 II - 输入有序数组</li></ol><p>给定一个已按照升序排列 的有序数组，找到两个数使得它们相加之和等于目标数。</p><p>函数应该返回这两个下标值 index1 和 index2，其中 index1 必须小于 index2。</p><p>说明:</p><p>返回的下标值（index1 和 index2）不是从零开始的。<br>你可以假设每个输入只对应唯一的答案，而且你不可以重复使用相同的元素。<br>示例:</p><p>输入: numbers = [2, 7, 11, 15], target = 9<br>输出: [1,2]<br>解释: 2 与 7 之和等于目标数 9 。因此 index1 = 1, index2 = 2 。</p><p>[思路]：使用两个指针，分别在一头一尾，比较这两个指针所对应的元素之和是否等于目标，等于则返回，小于目标，则移动头指针，反之，则移动尾指针。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Solution</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">twoSum</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> numbers<span class="token punctuation">:</span> List<span class="token punctuation">[</span>int<span class="token punctuation">]</span><span class="token punctuation">,</span> target<span class="token punctuation">:</span> int<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> List<span class="token punctuation">[</span>int<span class="token punctuation">]</span><span class="token punctuation">:</span>        left <span class="token operator">=</span> <span class="token number">0</span>        right <span class="token operator">=</span> len<span class="token punctuation">(</span>numbers<span class="token punctuation">)</span><span class="token operator">-</span><span class="token number">1</span>        <span class="token keyword">while</span> left <span class="token operator">&lt;</span> right<span class="token punctuation">:</span>            <span class="token keyword">if</span> numbers<span class="token punctuation">[</span>left<span class="token punctuation">]</span> <span class="token operator">+</span> numbers<span class="token punctuation">[</span>right<span class="token punctuation">]</span> <span class="token operator">==</span> target<span class="token punctuation">:</span>                <span class="token keyword">return</span> <span class="token punctuation">[</span>left<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> right<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span>            <span class="token keyword">elif</span> numbers<span class="token punctuation">[</span>left<span class="token punctuation">]</span> <span class="token operator">+</span> numbers<span class="token punctuation">[</span>right<span class="token punctuation">]</span> <span class="token operator">&lt;</span> target<span class="token punctuation">:</span>                left <span class="token operator">=</span> left <span class="token operator">+</span><span class="token number">1</span>            <span class="token keyword">else</span><span class="token punctuation">:</span>                right <span class="token operator">=</span> right <span class="token operator">-</span><span class="token number">1</span></code></pre><h5 id="2-堆排序"><a href="#2-堆排序" class="headerlink" title="2 堆排序"></a>2 堆排序</h5><ol start="215"><li>数组中的第K个最大元素</li></ol><p>在未排序的数组中找到第 k 个最大的元素。请注意，你需要找的是数组排序后的第 k 个最大的元素，而不是第 k 个不同的元素。</p><p>示例 1:</p><p>输入: [3,2,1,5,6,4] 和 k = 2<br>输出: 5<br>示例 2:</p><p>输入: [3,2,3,1,2,4,5,5,6] 和 k = 4<br>输出: 4<br>说明:</p><p>你可以假设 k 总是有效的，且 1 ≤ k ≤ 数组的长度。</p><p>[思路]：建立一个小顶堆，并且把前k个元素添加到小顶堆里，这个时候小顶堆已经默认排好了序，并且顶端是最小的元素，接着把剩下的n-k个元素与小顶堆比较，若是大于堆顶元素，则push进堆，这样，小顶堆就可以保存最大的k个元素，堆顶就是k个元素中最小的元素，也就是第k个最大的元素。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> heapq<span class="token keyword">class</span> <span class="token class-name">Solution</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">findKthLargest</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> nums<span class="token punctuation">:</span> List<span class="token punctuation">[</span>int<span class="token punctuation">]</span><span class="token punctuation">,</span> k<span class="token punctuation">:</span> int<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> int<span class="token punctuation">:</span>        heap  <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> num <span class="token keyword">in</span> nums<span class="token punctuation">[</span><span class="token punctuation">:</span>k<span class="token punctuation">]</span><span class="token punctuation">:</span>            heapq<span class="token punctuation">.</span>heappush<span class="token punctuation">(</span>heap<span class="token punctuation">,</span>num<span class="token punctuation">)</span>        <span class="token keyword">for</span> num <span class="token keyword">in</span> nums<span class="token punctuation">[</span>k<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> num <span class="token operator">></span> heap<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">:</span>                heapq<span class="token punctuation">.</span>heappop<span class="token punctuation">(</span>heap<span class="token punctuation">)</span>                heapq<span class="token punctuation">.</span>heappush<span class="token punctuation">(</span>heap<span class="token punctuation">,</span>num<span class="token punctuation">)</span>        <span class="token keyword">return</span> heap<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span></code></pre><h5 id="3-桶排序"><a href="#3-桶排序" class="headerlink" title="3 桶排序"></a>3 桶排序</h5><ol start="347"><li>前 K 个高频元素</li></ol><p>给定一个非空的整数数组，返回其中出现频率前 k 高的元素。</p><p>示例 1:</p><p>输入: nums = [1,1,1,2,2,3], k = 2<br>输出: [1,2]<br>示例 2:</p><p>输入: nums = [1], k = 1<br>输出: [1]<br>说明：</p><p>你可以假设给定的 k 总是合理的，且 1 ≤ k ≤ 数组中不相同的元素的个数。<br>你的算法的时间复杂度必须优于 O(n log n) , n 是数组的大小。</p><p>[思路] 根据元素的个数创建若干个桶， 初始值设置为0， 如果该元素重复出现，则桶中的元素加1，最后取出元素出现频率前k高的元素。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Solution</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">topKFrequent</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> nums<span class="token punctuation">:</span> List<span class="token punctuation">[</span>int<span class="token punctuation">]</span><span class="token punctuation">,</span> k<span class="token punctuation">:</span> int<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> List<span class="token punctuation">[</span>int<span class="token punctuation">]</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># 统计数组中元素出现的频率</span>        frequency_dict <span class="token operator">=</span> dict <span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 以数组中不重复的元素做为key，出现的频率做为value填充字典</span>        <span class="token keyword">for</span> num <span class="token keyword">in</span> nums<span class="token punctuation">:</span>            frequency_dict<span class="token punctuation">[</span>num<span class="token punctuation">]</span> <span class="token operator">=</span> frequency_dict<span class="token punctuation">.</span>get<span class="token punctuation">(</span>num<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span>        <span class="token comment" spellcheck="true"># 以数组中元素个数的数量创建对应的桶</span>        bucket <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token punctuation">]</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>nums<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> key<span class="token punctuation">,</span> value <span class="token keyword">in</span> frequency_dict<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            bucket<span class="token punctuation">[</span>value<span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>key<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 逆序取出前k个元素</span>        result <span class="token operator">=</span> list<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>nums<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> bucket<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">:</span>                result<span class="token punctuation">.</span>extend<span class="token punctuation">(</span>bucket<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>            <span class="token keyword">if</span> len<span class="token punctuation">(</span>result<span class="token punctuation">)</span> <span class="token operator">>=</span> k<span class="token punctuation">:</span>                <span class="token keyword">break</span>        <span class="token keyword">return</span> result<span class="token punctuation">[</span><span class="token punctuation">:</span>k<span class="token punctuation">]</span></code></pre><h5 id="4-荷兰国旗问题"><a href="#4-荷兰国旗问题" class="headerlink" title="4 荷兰国旗问题"></a>4 荷兰国旗问题</h5><ol start="75"><li>颜色分类</li></ol><p>给定一个包含红色、白色和蓝色，一共 n 个元素的数组，原地对它们进行排序，使得相同颜色的元素相邻，并按照红色、白色、蓝色顺序排列。</p><p>此题中，我们使用整数 0、 1 和 2 分别表示红色、白色和蓝色。</p><p>注意:<br>不能使用代码库中的排序函数来解决这道题。</p><p>示例:</p><p>输入: [2,0,2,1,1,0]<br>输出: [0,0,1,1,2,2]</p><p>[思路] 用3个指针，分别指向一头一尾和当前元素，根据当前指针判断球的颜色，如果是红色，则交换元素位置，同时头指针和当前的指针都往后移动一位，如果是白色，只移动当前指针，如果是蓝色，则交换元素位置，并且尾指针往前移动一位。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Solution</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">sortColors</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> nums<span class="token punctuation">:</span> List<span class="token punctuation">[</span>int<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> None<span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        Do not return anything, modify nums in-place instead.        """</span>        <span class="token comment" spellcheck="true"># 定义三个指针，left在最左边，curr代表当前的数据的指针，left指向最右边</span>        left <span class="token operator">=</span> curr <span class="token operator">=</span><span class="token number">0</span>        right <span class="token operator">=</span> len<span class="token punctuation">(</span>nums<span class="token punctuation">)</span><span class="token operator">-</span><span class="token number">1</span>        <span class="token keyword">while</span> curr <span class="token operator">&lt;=</span> right<span class="token punctuation">:</span>            <span class="token keyword">if</span> nums<span class="token punctuation">[</span>curr<span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">2</span><span class="token punctuation">:</span>                <span class="token comment" spellcheck="true"># 蓝球，则移动到最后</span>                nums<span class="token punctuation">[</span>curr<span class="token punctuation">]</span><span class="token punctuation">,</span>nums<span class="token punctuation">[</span>right<span class="token punctuation">]</span> <span class="token operator">=</span> nums<span class="token punctuation">[</span>right<span class="token punctuation">]</span><span class="token punctuation">,</span>nums<span class="token punctuation">[</span>curr<span class="token punctuation">]</span>                <span class="token comment" spellcheck="true"># 右指针往前移动一位</span>                right <span class="token operator">-=</span> <span class="token number">1</span>            <span class="token keyword">elif</span> nums<span class="token punctuation">[</span>curr<span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>                <span class="token comment" spellcheck="true"># 当前球是白球，当前指针往后移</span>                curr <span class="token operator">+=</span> <span class="token number">1</span>            <span class="token keyword">elif</span> nums<span class="token punctuation">[</span>curr<span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>                <span class="token comment" spellcheck="true"># 当前球是红球，则移动到最前</span>                nums<span class="token punctuation">[</span>left<span class="token punctuation">]</span><span class="token punctuation">,</span>nums<span class="token punctuation">[</span>curr<span class="token punctuation">]</span> <span class="token operator">=</span> nums<span class="token punctuation">[</span>curr<span class="token punctuation">]</span><span class="token punctuation">,</span>nums<span class="token punctuation">[</span>left<span class="token punctuation">]</span>                <span class="token comment" spellcheck="true"># 左指针和当前指针都往后移1位</span>                left <span class="token operator">+=</span> <span class="token number">1</span>                curr <span class="token operator">+=</span><span class="token number">1</span></code></pre><h5 id="5-贪心算法"><a href="#5-贪心算法" class="headerlink" title="5 贪心算法"></a>5 贪心算法</h5><ol start="455"><li>分发饼干</li></ol><p>假设你是一位很棒的家长，想要给你的孩子们一些小饼干。但是，每个孩子最多只能给一块饼干。对每个孩子 i ，都有一个胃口值 gi ，这是能让孩子们满足胃口的饼干的最小尺寸；并且每块饼干 j ，都有一个尺寸 sj 。如果 sj &gt;= gi ，我们可以将这个饼干 j 分配给孩子 i ，这个孩子会得到满足。你的目标是尽可能满足越多数量的孩子，并输出这个最大数值。</p><p>注意：</p><p>你可以假设胃口值为正。<br>一个小朋友最多只能拥有一块饼干。</p><p>示例 1:</p><p>输入: [1,2,3], [1,1]</p><p>输出: 1</p><p>解释:<br>你有三个孩子和两块小饼干，3个孩子的胃口值分别是：1,2,3。<br>虽然你有两块小饼干，由于他们的尺寸都是1，你只能让胃口值是1的孩子满足。<br>所以你应该输出1。<br>示例 2:</p><p>输入: [1,2], [1,2,3]</p><p>输出: 2</p><p>解释:<br>你有两个孩子和三块小饼干，2个孩子的胃口值分别是1,2。<br>你拥有的饼干数量和尺寸都足以让所有孩子满足。<br>所以你应该输出2.</p><p>[思路] 先对小孩胃口和饼干尺寸按照从小到大排序，定义两个指针分别指向小孩胃口和饼干尺寸，对于每个小孩来说，如果饼干尺寸满足了小孩胃口，继续判断下一个小孩和下一块饼干尺寸，如果饼干尺寸没有满足小孩胃口，则指向小孩胃口的指针不动，只是移动饼干尺寸的指针。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Solution</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">findContentChildren</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> g<span class="token punctuation">:</span> List<span class="token punctuation">[</span>int<span class="token punctuation">]</span><span class="token punctuation">,</span> s<span class="token punctuation">:</span> List<span class="token punctuation">[</span>int<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> int<span class="token punctuation">:</span>        g<span class="token punctuation">.</span>sort<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#小孩按照胃口从小到大排序</span>        s<span class="token punctuation">.</span>sort<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#饼干按照从小到大排序</span>        g_<span class="token punctuation">,</span> s_<span class="token punctuation">,</span> count <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span> <span class="token comment" spellcheck="true">#初始化小孩指针，饼干指针和计数器</span>        <span class="token keyword">while</span> g_ <span class="token operator">&lt;</span> len<span class="token punctuation">(</span>g<span class="token punctuation">)</span> <span class="token operator">and</span> s_ <span class="token operator">&lt;</span> len<span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> g<span class="token punctuation">[</span>g_<span class="token punctuation">]</span> <span class="token operator">&lt;=</span> s<span class="token punctuation">[</span>s_<span class="token punctuation">]</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 当前饼干满足了小孩的胃口</span>                count <span class="token operator">+=</span><span class="token number">1</span>       <span class="token comment" spellcheck="true"># 计数器+1</span>                g_ <span class="token operator">+=</span> <span class="token number">1</span>         <span class="token comment" spellcheck="true"># 下一个小孩</span>                s_ <span class="token operator">+=</span> <span class="token number">1</span>         <span class="token comment" spellcheck="true"># 下一块饼干</span>            <span class="token keyword">else</span><span class="token punctuation">:</span>                               s_ <span class="token operator">+=</span> <span class="token number">1</span>         <span class="token comment" spellcheck="true"># 没有满足，则查看下一块饼干</span>        <span class="token keyword">return</span> count</code></pre>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 排序 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>这！就是深度学习之反向传播算法(四十)</title>
      <link href="/2019/08/25/dl40/"/>
      <url>/2019/08/25/dl40/</url>
      
        <content type="html"><![CDATA[<p>前面的第二十七和二十八篇简单的介绍了一下神经网络，不过对于其中的BP算法，还是有点不太理解，所以这次举两个例子，期望加深理解。</p><h4 id="反向传播（Back-Propagation）算法"><a href="#反向传播（Back-Propagation）算法" class="headerlink" title="反向传播（Back Propagation）算法"></a>反向传播（Back Propagation）算法</h4><p>当我们使用前馈神经网络接受输入x并产生输出$\hat {y}$时，信息通过网络向前流动。输入x提供的初始信息，然后传播到每一层的隐藏单元，最终产生输出$\hat {y}$，这称之为前向传播（forward propagation）。在训练过程中，前向传播可以持续向前直到它产生一个标量代价函数$J( \theta )$。反向传播（back propagation）算法，简称BP算法，允许来自代价函数的信息通过网络向后流动，以便计算梯度。BP算法基于梯度下降的策略，以目标为负梯度方向对参数进行调整。</p><h4 id="抽象的例子"><a href="#抽象的例子" class="headerlink" title="抽象的例子"></a>抽象的例子</h4><p>假设有下图所示的神经网络<br><img src="/2019/08/25/dl40/dl4001.jpg" alt="反向传播"></p><p>假设X为$N*m$的矩阵，其中N为样本的个数，m为特征数。W为权重的矩阵，f为作<br>用在h上的激活函数，$\tilde b$为$b_1^T$后沿着行方向的扩展，那么根据前向传播算法，可得：</p><p>$out =Z_LW_{L+1}+\tilde b_{L+1}$<br>我们假设当前的损失函数为J，因为反向传播算法的目的是要求神经网络模型的输出相对于各个参数的梯度值，即求$\frac {\partial J}{\partial out}$.</p><p>我们把上面的假设稍微具化一下，令：<br>$$Z_L=\begin{pmatrix}<br>z_{11} &amp; z_{12} &amp; z_{13} \\<br>z_{21} &amp; z_{22} &amp; z_{23}<br>\end{pmatrix},<br>W_{L+1}=\begin{pmatrix}<br>w_{11} &amp; w_{12} \\<br>w_{21} &amp; w_{22} \\<br>w_{31} &amp; w_{32}<br>\end{pmatrix},<br>\tilde b_{L+1}=\begin{pmatrix}<br>b_{1} &amp; b_{1} \\<br>b_{2} &amp; b_{2}<br>\end{pmatrix}$$<br>$$out=\begin{pmatrix}<br>o_{11} &amp; o_{12} \\<br>o_{21} &amp; o_{22}<br>\end{pmatrix}$$<br>则：<br>$$Z_L \times W_{L+1}=\begin{pmatrix}<br>z_{11}w_{11}+z_{12}w_{21}+z_{13}w_{31} &amp; z_{11}w_{12}+z_{12}w_{22}+z_{13}w_{32} \\<br>z_{21}w_{11}+z_{22}w_{21}+z_{23}w_{31} &amp; z_{21}w_{12}+z_{22}w_{22}+z_{23}w_{32}<br>\end{pmatrix}$$<br>所以：<br>$$o_{11} = z_{11}w_{11}+z_{12}w_{21}+z_{13}w_{31} +b_1$$<br>$$o_{12} = z_{11}w_{12}+z_{12}w_{22}+z_{13}w_{32} +b_2$$<br>$$o_{21} = z_{21}w_{11}+z_{22}w_{21}+z_{23}w_{31} +b_1$$<br>$$o_{22} = z_{21}w_{12}+z_{22}w_{22}+z_{23}w_{32} +b_2$$</p><h5 id="对w的参数求偏导"><a href="#对w的参数求偏导" class="headerlink" title="对w的参数求偏导"></a>对w的参数求偏导</h5><p>根据链式求导法则（J -&gt; o -&gt; w）有：<br>$$\frac {\partial J}{\partial w_{11}}=\frac {\partial J} {\partial o_{11}}z_{11} + \frac {\partial J} {\partial o_{21}}z_{21}$$<br>$$\frac {\partial J}{\partial w_{12}}=\frac {\partial J} {\partial o_{12}}z_{11} + \frac {\partial J} {\partial o_{22}}z_{21}$$<br>$$\frac {\partial J}{\partial w_{21}}=\frac {\partial J} {\partial o_{11}}z_{12} + \frac {\partial J} {\partial o_{21}}z_{22}$$<br>$$\frac {\partial J}{\partial w_{22}}=\frac {\partial J} {\partial o_{12}}z_{12} + \frac {\partial J} {\partial o_{22}}z_{22}$$<br>$$\frac {\partial J}{\partial w_{31}}=\frac {\partial J} {\partial o_{11}}z_{13} + \frac {\partial J} {\partial o_{21}}z_{23}$$<br>$$\frac {\partial J}{\partial w_{32}}=\frac {\partial J} {\partial o_{11}}z_{13} + \frac {\partial J} {\partial o_{22}}z_{23}$$<br>用矩阵表示就是：<br>$$\begin{pmatrix}<br>\frac {\partial J}{\partial w_{11}} &amp; \frac {\partial J}{\partial w_{12}} \\<br>\frac {\partial J}{\partial w_{21}} &amp; \frac {\partial J}{\partial w_{22}} \\<br>\frac {\partial J}{\partial w_{31}} &amp; \frac {\partial J}{\partial w_{32}}<br>\end{pmatrix} =<br>\begin{pmatrix}<br>z_{11} &amp; z_{21} \\<br>z_{21} &amp; z_{22} \\<br>z_{31} &amp; z_{32}<br>\end{pmatrix}<br>\begin{pmatrix}<br>\frac {\partial J} {\partial o_{11}} &amp; \frac {\partial J} {\partial o_{12}} \\<br>\frac {\partial J} {\partial o_{21}} &amp; \frac {\partial J} {\partial o_{22}}<br>\end{pmatrix}$$<br>最左边的矩阵也叫雅克比（jacobian）矩阵。所以：<br>$$\frac {\partial J} {\partial W_{L+1}} = Z_L^T \frac {\partial J} {\partial out}$$</p><h5 id="对bias参数b求偏导"><a href="#对bias参数b求偏导" class="headerlink" title="对bias参数b求偏导"></a>对bias参数b求偏导</h5><p>$$\frac {\partial J} {\partial b_1} = \frac {\partial J} {\partial o_{11}} + \frac {\partial J} {\partial o_{21}}$$<br>$$\frac {\partial J} {\partial b_2} = \frac {\partial J} {\partial o_{12}} + \frac {\partial J} {\partial o_{22}}$$<br>用矩阵表示：<br>$$(\frac {\partial J} {\partial b})^T = (\frac {\partial J} {\partial b_1} \   \frac {\partial J} {\partial b_2}) =(\frac {\partial J} {\partial o_{11}} + \frac {\partial J} {\partial o_{21}} \   \frac {\partial J} {\partial o_{12}} + \frac {\partial J} {\partial o_{22}})$$<br>也就是将$\frac {\partial J} {\partial out}$的每一行都加起来。</p><p>上面的例子中，我们求的是$W_{L+1}的梯度$，既然是反向传播，我们接下来就要求$W_L$的梯度，根据上面的公式，我们可以得到：<br>$$\frac {\partial J} {\partial W_L} = Z_{L-1}^T \frac {\partial J} {\partial h_L}$$<br>那么现在的问题是怎么求$\frac {\partial J} {\partial h_L}$呢？通过图片可以看到$h_L$经过激活函数的变换到了$Z_L$, 那么同样的根据链式求导法则，我们先求出$Z_L$的梯度，再求出激活函数$f_L$的梯度，也就求出了$h_L$的梯度，即$\frac {\partial J} {\partial h_L}$</p><h5 id="对Z求偏导"><a href="#对Z求偏导" class="headerlink" title="对Z求偏导"></a>对Z求偏导</h5><p>$$\frac {\partial J}{\partial z_{11}}=\frac {\partial J} {\partial o_{11}}w_{11} + \frac {\partial J} {\partial o_{12}}w_{12}$$<br>$$\frac {\partial J}{\partial z_{12}}=\frac {\partial J} {\partial o_{11}}w_{21} + \frac {\partial J} {\partial o_{12}}w_{22}$$<br>$$\frac {\partial J}{\partial z_{13}}=\frac {\partial J} {\partial o_{11}}w_{31} + \frac {\partial J} {\partial o_{12}}w_{32}$$<br>$$\frac {\partial J}{\partial z_{21}}=\frac {\partial J} {\partial o_{21}}w_{11} + \frac {\partial J} {\partial o_{12}}w_{12}$$<br>$$\frac {\partial J}{\partial z_{22}}=\frac {\partial J} {\partial o_{21}}w_{21} + \frac {\partial J} {\partial o_{22}}w_{22}$$<br>$$\frac {\partial J}{\partial z_{23}}=\frac {\partial J} {\partial o_{21}}w_{31} + \frac {\partial J} {\partial o_{22}}w_{32}$$<br>用矩阵表示：<br>$$\begin{pmatrix}<br>\frac {\partial J}{\partial z_{11}} &amp; \frac {\partial J}{\partial z_{12}} &amp; \frac {\partial J}{\partial z_{13}} \\<br>\frac {\partial J}{\partial z_{21}} &amp; \frac {\partial J}{\partial z_{22}}&amp; \frac {\partial J}{\partial z_{23}}<br>\end{pmatrix} =<br>\begin{pmatrix}<br>\frac {\partial J} {\partial o_{11}} &amp; \frac {\partial J} {\partial o_{12}} \\<br>\frac {\partial J} {\partial o_{21}} &amp; \frac {\partial J} {\partial o_{22}}<br>\end{pmatrix}<br>\begin{pmatrix}<br>w_{11} &amp; w_{12} &amp; w_{31} \\<br>w_{21} &amp; w_{22} &amp; w_{33}<br>\end{pmatrix}$$<br>即：<br>$$\frac {\partial J} {\partial Z_L} = \frac {\partial J} {\partial out} W_{L+1}^T $$</p><h5 id="对激活函数求偏导"><a href="#对激活函数求偏导" class="headerlink" title="对激活函数求偏导"></a>对激活函数求偏导</h5><p>因为$f_L$是作用于$h_L$的激活函数，所以$Z_L = f_L (h_L)$</p><h6 id="1-激活函数为sigmoid"><a href="#1-激活函数为sigmoid" class="headerlink" title="1 激活函数为sigmoid"></a>1 激活函数为sigmoid</h6><p>$$Z_L = \frac {1} {1+e^{-h_L}}$$<br>复合函数求偏导：<br>$$\frac {\partial J} {\partial h_L} = \frac {\partial J} {\partial Z_L} \frac {\partial Z_L} {\partial h_L}$$<br>$$=\frac {\partial J} {\partial Z_L} \frac {e^{-h_L}} {(1+e^{-h_L})^2}$$<br>$$=\frac {\partial J} {\partial Z_L} \frac {1} {(1+e^{-h_L})}\frac {e^{-h_L}} {(1+e^{-h_L})}$$<br>$$=\frac {\partial J} {\partial Z_L} Z_L (1-Z_L)$$</p><h6 id="2-激活函数为sigmoid"><a href="#2-激活函数为sigmoid" class="headerlink" title="2 激活函数为sigmoid"></a>2 激活函数为sigmoid</h6><p>$$Z_L = \frac {e^{h_L} - e^{-h_L}} {e^{h_L} + e^{-h_L}}$$<br>$$\frac {\partial J} {\partial h_L} = \frac {\partial J} {\partial Z_L} \frac {\partial Z_L} {\partial h_L}$$<br>因为$\frac {\partial Z_L} {\partial h_L}$求导比较复杂，所以单拎出来说。<br>令$a=e^{h_L}, b=e^{-h_L}$<br>根据复合函数求导公式：<br>$$(\frac {u} {v})’ = \frac {u’v-uv’} {v^2}$$<br>再令$u=(a-b), v=(a+b)$, 则有：<br>$$\frac {\partial J} {\partial h_L} = \left (\frac {a-b} {a+b} \right )’$$<br>$$=\frac {(a-b)’(a+b)-(a-b)(a+b)’} {(a+b)^2}$$<br>而：<br>$$(a-b)’=(e^{h_L} - e^{-h_L})’=e^{h_L}-(-1)e^{-h_L}=e^{h_L}+e^{-h_L}=a+b$$<br>$$(a+b)’=(e^{h_L} +e^{-h_L})’=e^{h_L}+(-1)e^{-h_L}=e^{h_L}-e^{-h_L}=a-b$$<br>带回原式可得：<br>$$\frac {(a-b)’(a+b)-(a-b)(a+b)’} {(a+b)^2}$$<br>$$=\frac {(a+b)^2-(a-b)^2} {(a+b)^2}$$<br>$$=1-\frac {(a-b)^2} {(a+b)^2}$$<br>$$=1-\left (\frac {a-b} {a+b} \right )^2$$<br>$$=1-\left (\frac {e^{h_L} - e^{-h_L}} {e^{h_L} + e^{-h_L}} \right )^2$$<br>所以<br>$$\frac {\partial J} {\partial h_L} = \frac {\partial J} {\partial Z_L} (1-\left (\frac {e^{h_L} - e^{-h_L}} {e^{h_L} + e^{-h_L}} \right )^2)$$<br>$$= \frac {\partial J} {\partial Z_L} (1-Z_L^2)$$</p><h6 id="3-激活函数为ReLU"><a href="#3-激活函数为ReLU" class="headerlink" title="3 激活函数为ReLU"></a>3 激活函数为ReLU</h6><p>$$Z_L=ReLU(h_L)=\left\{\begin{matrix}<br>0,\ h_L \leq 0 \\<br>h_L,\ h_L &gt;0<br>\end{matrix}\right.$$<br>对$h_L$求偏导可得：<br>$$\frac {\partial J} {\partial h_L} = \frac {\partial J} {\partial Z_L} \frac {\partial Z_L} {\partial h_L}=\left\{\begin{matrix}<br>0,\ h_L \leq 0 \\<br>\frac {\partial J} {\partial Z_L},\ h_L &gt;0<br>\end{matrix}\right.$$</p><h4 id="形象的例子"><a href="#形象的例子" class="headerlink" title="形象的例子"></a>形象的例子</h4><p>再举个形象点的例子，如下图：<br><img src="/2019/08/25/dl40/dl4002.jpg" alt="反向传播"><br>权重和偏执已经在图中标出来了，我们假设$x_1=1,x_2=4,x_3=5$, 最后的真实结果$t_1=0.1,t_2=0.05$</p><h5 id="1-输入层到隐藏层"><a href="#1-输入层到隐藏层" class="headerlink" title="1 输入层到隐藏层"></a>1 输入层到隐藏层</h5><p>$$z_{h_1}=w_1x_1+w_3x_2+w_5x_3+b_1$$<br>$$z_{h_2}=w_2x_1+w_4x_2+w_6x_3+b_1$$<br>$$h1 = \sigma(z_{h_1})$$<br>$$h2 = \sigma(z_{h_2})$$</p><h5 id="2-隐藏层到输出层"><a href="#2-隐藏层到输出层" class="headerlink" title="2 隐藏层到输出层"></a>2 隐藏层到输出层</h5><p>$$z_{o_1}=w_7h_1+w_9h_2+b_2$$<br>$$z_{o_2}=w_8h_1+w_10h_2+b_2$$<br>$$o1 = \sigma(z_{o_1})$$<br>$$o2 = \sigma(z_{o_2})$$</p><h5 id="3-计算hidden-layer-和-output-layer的值"><a href="#3-计算hidden-layer-和-output-layer的值" class="headerlink" title="3 计算hidden layer 和 output layer的值"></a>3 计算hidden layer 和 output layer的值</h5><p>这里的激活函数用的是sigmoid。<br>$$z_{h_1}=w_1x_1+w_3x_2+w_5x_3+b_1=0.1 \times 1 + 0.3 \times 4 + 0.5 \times 5 + 0.5 = 4.3 $$<br>$$h1 = \sigma(z_{h_1}) = \sigma (4.3) = 0.9866$$<br>$$z_{h_2}=w_2x_1+w_4x_2+w_6x_3+b_1=0.2 \times 1 + 0.4 \times 4 + 0.6 \times 5 + 0.5 = 5.3$$<br>$$h2 = \sigma(z_{h_2}) = \sigma (5.3) = 0.9950$$<br>$$z_{o_1}=w_7h_1+w_9h_2+b_2 = 0.7 \times 0.9866 + 0.9 \times 0.9950 + 0.5 = 2.0862$$<br>$$o1 = \sigma(z_{o_1}) = \sigma (2.0862) = 0.8896$$<br>$$z_{o_1}=w_7h_1+w_9h_2+b_2 = 0.8 \times 0.9866 + 0.1 \times 0.9950 + 0.5 = 1.3888$$<br>$$o1 = \sigma(z_{o_1}) = \sigma (1.3888) = 0.8004$$</p><h5 id="4-用计算误差"><a href="#4-用计算误差" class="headerlink" title="4 用计算误差"></a>4 用计算误差</h5><p>这里用平方和来计算损失值，记做E。<br>$$E =\frac{1}{2} \left (\ (o_1 - t_1)^2 +(o_2 - t_2)^2 \right )$$<br>所以有：<br>$$\frac {dE} {d_{o_1}} = o_1 - t_1$$<br>$$\frac {dE} {d_{o_2}} = o_2 - t_2$$</p><h5 id="5-反向传播之倒一层"><a href="#5-反向传播之倒一层" class="headerlink" title="5 反向传播之倒一层"></a>5 反向传播之倒一层</h5><p>因为<br>$$z_{o_1}=w_7h_1+w_9h_2+b_2$$<br>$$z_{o_2}=w_8h_1+w_10h_2+b_2$$<br>所以<br>$\frac {dz_{o_1}} {dw_7}=h_1,\frac {dz_{o_2}} {dw_8}=h_1,\frac {dz_{o_1}} {dw_9}=h_2,\frac {dz_{o_2}} {dw_10}=h_2,\frac {dz_{o_1}} {db_2}=1,\frac {dz_{o_2}} {db_2}=1$<br>根据链式求导法则：<br>$\frac {dE} {dw_7}=\frac {dE} {do_1} \frac {do_1} {dz_{o_1}} \frac {dz_{o_1}} {dw_7} = (o_1 - t_1)(o_1(1-o_1))h_1$<br>$=(0.8896-0.1)(0.8896(1-0.8896))0.9866=0.0765$<br>同理可得：<br>$\frac {dE} {dw_8}=\frac {dE} {do_2} \frac {do_2} {dz_{o_2}} \frac {dz_{o_2}} {dw_8} =0.7504 \times 0.1598 \times 0.9866 = 0.1183$<br>$\frac {dE} {dw_9}=\frac {dE} {do_1} \frac {do_1} {dz_{o_1}} \frac {dz_{o_1}} {dw_9} =0.7896 \times 0.0983 \times 0.9950 = 0.0772$<br>$\frac {dE} {dw_10}=\frac {dE} {do_2} \frac {do_2} {dz_{o_2}} \frac {dz_{o_2}} {dw_10} =0.7504 \times 0.1598 \times 0.9950 = 0.1193$<br>$\frac {dE} {db_2}=\frac {dE} {do_1} \frac {do_1} {dz_{o_1}} \frac {dz_{o_1}} {db_2} + \frac {dE} {do_2} \frac {do_2} {dz_{o_2}} \frac {dz_{o_2}} {db_2}$<br>$=0.7896 \times 0.0983 \times 1 + 0.7504 \times 0.1598 \times 1 =0.1975$</p><h5 id="6-反向传播之倒二层"><a href="#6-反向传播之倒二层" class="headerlink" title="6 反向传播之倒二层"></a>6 反向传播之倒二层</h5><p>上面已经求出了$\frac {dE} {dw_7},\frac {dE} {dw_8},\frac {dE} {dw_9},\frac {dE} {dw_10},\frac {dE} {db_2}$,我们接着求 $\frac {dE} {dw_1},\frac {dE} {dw_2},\frac {dE} {dw_3},\frac {dE} {dw_4},\frac {dE} {dw_5},\frac {dE} {dw_6},\frac {dE} {db_1}$<br>根据链式求导法则：<br>$\frac {dE} {dw_1} = \frac {dE} {dh_1} \frac {dh_1} {dz_{h_1}} \frac {dz_{h_1}} {dw_1}$<br>在对$h_1$求导的时候，因为$h_1$同时作用于$o_1$和$o_2$，所以<br>$\frac {dE} {dh_1} =\frac {dE} {do_1} \frac {do_1} {dz_{o_1}} \frac {dz_{o_1}} {dh_1} + \frac {dE} {do_2} \frac {do_2} {dz_{o_2}} \frac {dz_{o_2}} {dh_1}$<br>$=0.7896 \times 0.0983 \times 0.7 + 0.7504 \times 0.1598 \times 0.8 = 0.1502$<br>从而求得：<br>$\frac {dE} {dw_1} = 0.1502 \times 0.0132 \times 1 = 0.0020$<br>同理求得：<br>$\frac {dE} {dw_3} = \frac {dE} {dh_1} \frac {dh_1} {dz_{h_1}} \frac {dz_{h_1}} {dw_3} =0.1502 \times 0.0132 \times 4 = 0.0079$<br>$\frac {dE} {dw_5} = \frac {dE} {dh_1} \frac {dh_1} {dz_{h_1}} \frac {dz_{h_1}} {dw_5} =0.1502 \times 0.0132 \times 5 = 0.0099$<br>接着求$h_2$节点对应的权重值：<br>$\frac {dE} {dw_2} = \frac {dE} {dh_2} \frac {dh_2} {dz_{h_2}} \frac {dz_{h_2}} {dw_2}$<br>而：<br>$\frac {dE} {dh_2} =\frac {dE} {do_1} \frac {do_1} {dz_{o_1}} \frac {dz_{o_1}} {dh_2} + \frac {dE} {do_2} \frac {do_2} {dz_{o_2}} \frac {dz_{o_2}} {dh_2}$<br>$=0.7896 \times 0.0983 \times 0.9 + 0.7504 \times 0.1598 \times 0.1 = 0.0818$<br>从而：<br>$\frac {dE} {dw_2} =0.0818 \times 0.0049\times 1 = 0.0004$</p><p>$\frac {dE} {dw_4}=\frac {dE} {dh_2} \frac {dh_2} {dz_{h_2}} \frac {dz_{h_2}} {dw_4} =0.0818 \times 0.0049 \times 4 = 0.0016$<br>$\frac {dE} {dw_6}=\frac {dE} {dh_2} \frac {dh_2} {dz_{h_2}} \frac {dz_{h_2}} {dw_6} =0.0818 \times 0.0049 \times 5 = 0.0020$</p><p>$\frac {dE} {db_1}=\frac {dE} {do_1} \frac {do_1} {dz_{o_1}} \frac {dz_{o_1}} {dh_1} \frac {dh_1} {dz_{h_1}} \frac {dz_{h_1}} {db_1} + \frac {dE} {do_2} \frac {do_2} {dz_{o_2}} \frac {dz_{o_2}} {dh_2} \frac {dh_2} {dz_{h_2}} \frac {dz_{2_1}} {db_1}$<br>$= 0.7896 \times 0.9983 \times 0.7 \times 0.0132 \times 1+ \times 0.7504 \times 0.1598 \times 0.1 \times 0.0049 \times 1= 0.0008$</p><p>然后我们开始利用梯度下降法求解，令学习率$\alpha=0.01$, 则有：<br>$w_1:=w_1-\alpha \frac {dE} {dw_1} = 0.1 - 0.01 \times 0.0020 = 0.1000$<br>$w_2:=w_2-\alpha \frac {dE} {dw_2} = 0.2 - 0.01 \times 0.0004 = 0.2000$<br>$w_3:=w_3-\alpha \frac {dE} {dw_3} = 0.3 - 0.01 \times 0.0079 = 0.2999$<br>$w_4:=w_4-\alpha \frac {dE} {dw_4} = 0.4 - 0.01 \times 0.0016 = 0.4000$<br>$w_5:=w_5-\alpha \frac {dE} {dw_5} = 0.5 - 0.01 \times 0.0099 = 0.4999$<br>$w_6:=w_6-\alpha \frac {dE} {dw_6} = 0.6 - 0.01 \times 0.0020 = 0.6000$<br>$w_7:=w_7-\alpha \frac {dE} {dw_7} = 0.7 - 0.01 \times 0.0765 = 0.6992$<br>$w_8:=w_8-\alpha \frac {dE} {dw_8} = 0.8 - 0.01 \times 0.1183 = 0.7988$<br>$w_9:=w_9-\alpha \frac {dE} {dw_9} = 0.9 - 0.01 \times 0.0772 = 0.8992$<br>$w_10:=w_10-\alpha \frac {dE} {dw_10} = 0.1 - 0.01 \times 0.1193 = 0.0988$<br>$b_1:=b_1-\alpha \frac {dE} {db_1} = 0.1 - 0.01 \times 0.0008 = 0.0500$<br>$b_1:=b_2-\alpha \frac {dE} {db_2} = 0.1 - 0.01 \times 0.1975 = 0.4980$</p><p>然后执行N次，直到找到下山的路，也就是误差降到最小值。</p><h5 id="7-Python实现上面的代码"><a href="#7-Python实现上面的代码" class="headerlink" title="7 Python实现上面的代码"></a>7 Python实现上面的代码</h5><p><img src="/2019/08/25/dl40/dl4006.jpg" alt="Python实现BP"><br>可以看到大约1500次左右，已经收敛到最小误差了。</p><h4 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h4><p>这部分，主要说几个常用的激活函数。</p><h6 id="1-sigmoid"><a href="#1-sigmoid" class="headerlink" title="1 sigmoid"></a>1 sigmoid</h6><p><img src="/2019/08/25/dl40/dl4003.jpg" alt="sigmoid"><br>优点：<br>1 它能够把输入的连续实值变换为0和1之间的输出，特别的，如果是非常大的负数，那么输出就是0；如果是非常大的正数，输出就是1.<br>缺点：<br>1 在深度神经网络中梯度反向传递时导致梯度爆炸和梯度消失，其中梯度爆炸发生的概率非常小，而梯度消失发生的概率比较大。</p><h6 id="2-tanh"><a href="#2-tanh" class="headerlink" title="2 tanh"></a>2 tanh</h6><p><img src="/2019/08/25/dl40/dl4004.jpg" alt="tanh"><br>优点：<br>1 它解决了Sigmoid函数的不是zero-centered输出问题。<br>2 它的导数包含了原函数的值, 所以多次求导的时候, 只需要预先算一次, 后面可以重复利用。<br>缺点：<br>1 梯度消失（gradient vanishing）的问题和幂运算的问题仍然存在。</p><h6 id="3-ReLU"><a href="#3-ReLU" class="headerlink" title="3 ReLU"></a>3 ReLU</h6><p><img src="/2019/08/25/dl40/dl4005.jpg" alt="ReLU"><br>优点：<br>1 解决了gradient vanishing问题 (在正区间)<br>2 计算速度非常快，只需要判断输入是否大于<br>3 收敛速度远快于sigmoid和tanh<br>缺点：<br>1 ReLU的输出不是zero-centered<br>2 Dead ReLU Problem，指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。</p><h6 id="4-Softmax"><a href="#4-Softmax" class="headerlink" title="4 Softmax"></a>4 Softmax</h6><p>公式为：<br>$$\sigma (z)_j= \frac {e^{z_j}} {\sum_{k=1}^K e^{z_k}}$$<br>将K维向量z, 映射到一个新的K维向量σ(z)上, 每个元素的值都∈(0,1),且加起来等于1, 且映射前后元素间的相对大小保持不变.<br>$\sigma (z)_j$表示新向量的第 j 个元素, $z_j$表示原向量的第 j 个元素. </p><p>一般与交叉熵损失函数搭配, 用于二分类, 多分类。</p><h6 id="5-Softplus"><a href="#5-Softplus" class="headerlink" title="5 Softplus"></a>5 Softplus</h6><p>公式为：<br>$$\zeta = log(1+e^x)$$<br>平滑版的ReLU函数，用的较少。</p><h6 id="6-Maxout"><a href="#6-Maxout" class="headerlink" title="6 Maxout"></a>6 Maxout</h6><p>公式为：<br>$$h_i(x)= \underset {j \in [1,k]}{max} z_{ij}$$<br>$$z_{ij}=x^TW_{…ij}+b_{ij}$$<br>比如说：<br>$$z_1=w_1X+b_1$$<br>$$z_2=w_2X+b_2$$<br>$$z_3=w_3X+b_3$$<br>$$out=max(z_1, z_2, z_3)$$<br>采用此种激活函数，计算参数会造成k倍增加。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BP </tag>
            
            <tag> DFN </tag>
            
            <tag> MLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>这！就是深度学习之机器学习基础(三十九)</title>
      <link href="/2019/08/25/dl39/"/>
      <url>/2019/08/25/dl39/</url>
      
        <content type="html"><![CDATA[<p>再次回顾线性回归和逻辑回归，是为了可以把知识串起来，形成体系。</p><h4 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h4><h5 id="模型公式"><a href="#模型公式" class="headerlink" title="模型公式"></a>模型公式</h5><p>线性回归的模型如下：<br>$$<br>h_\theta(x_1,x_2,\cdots,x_n)=\theta_0x_0 + \theta_1x_1 + \theta_2x_2 + \dots + \theta_nx_n = \sum_{i=0}^n\theta_ix_i<br>$$<br>上面的式子中，我们令 $x_0=1$ ，此时，$\theta = b$</p><p>用矩阵表示为：<br>$$<br>h_\theta(X)=\theta^TX<br>$$<br>其中，X为$m \times n$维矩阵，m为样本个数，n为每个样本的特征数。</p><p>一般情况下，样本的真实值和预测值之间有个误差$\epsilon$,表示为：<br>$$<br>Y = \theta^TX+\epsilon<br>$$<br>上式中，$\epsilon$是$m \times 1$维向量，代表m个样本相对于线性回归方程的上下浮动程度。$\epsilon$是独立同分布的，并且服从均值为0，方差为$\sigma^2$的正太分布。</p><h5 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h5><p>根据上面的公式，对于每个样本来说：<br>$$<br>\epsilon^{(j)}=y^{(j)}-\theta^Tx^{(j)} , j \in (1,2,\cdots,m)<br>$$<br>因为$\epsilon$是独立同分布的，并且服从均值为0，方差为$\sigma^2$的正太分布，所以<br>$$<br>f(\epsilon^{(j)})=\frac {1} {\sqrt {2 \pi} \sigma}e^{-\frac {(\epsilon^{(j)})^2} {2 \sigma^2}}<br>$$</p><p>$$<br>=\frac {1} {\sqrt {2 \pi} \sigma}e^{-\frac {(y^{(j)}-\theta^Tx^{(j)})^2} {2 \sigma^2}}<br>$$</p><p>引入似然函数：<br>$$<br>L(\theta)=\prod_{j=1}^m f(y^{(j)}|x^{(j)};\theta)=\prod_{j=1}^m \frac {1} {\sqrt {2 \pi} \sigma}e^{-\frac {(y^{(j)}-\theta^Tx^{(j)})^2} {2 \sigma^2}}<br>$$<br>两边同时取对数，令$l(\theta) = logL(\theta)$,可得：<br>$$<br>l(\theta) = log\prod_{j=1}^m \frac {1} {\sqrt {2 \pi} \sigma}e^{-\frac {(y^{(j)}-\theta^Tx^{(j)})^2} {2 \sigma^2}}<br>$$</p><p>$$<br>=\sum_{j=1}^mlog \frac {1} {\sqrt {2 \pi} \sigma}e^{-\frac {(y^{(j)}-\theta^Tx^{(j)})^2} {2 \sigma^2}}<br>$$</p><p>$$<br>=\sum_{j=1}^mlog \frac{1}{\sqrt {2 \pi} \sigma} + \sum_{j=1}^mlog e^{-\frac {(y^{(j)}-\theta^Tx^{(j)})^2} {2 \sigma^2}}<br>$$</p><p>$$<br>=mlog \frac{1}{\sqrt {2 \pi} \sigma} + \sum_{j=1}^m -\frac {(y^{(j)}-\theta^Tx^{(j)})^2} {2 \sigma^2}<br>$$</p><p>$$<br>= mlog \frac{1}{\sqrt {2 \pi} \sigma} - \frac{1}{\sigma^2} \frac{1}{2} \sum_{j=1}^m (y^{(j)}-\theta^Tx^{(j)})^2<br>$$</p><p>上面的式子是求极大似然函数的最大值，但是去除常数项，和负号之后，它就转为了求<br>$$<br>J(\theta) = \frac{1}{2} \sum_{j=1}^m (y^{(j)}-\theta^Tx^{(j)})^2<br>$$<br>的最小值，这也就是最小二乘法的由来。</p><p>把上式用矩阵表示：<br>$$<br>J(\theta) = \frac{1}{2} (X\theta - Y)^T(X\theta - Y)<br>$$</p><p>$$<br>=\frac{1}{2}(\theta^TX^T-Y^T)(X\theta - Y)<br>$$</p><p>$$<br>=\frac{1}{2}(\theta^T X^T X \theta -Y^T X\theta -\theta^T X^T Y +Y^TY)<br>$$</p><p>对上式求导数：<br>$$<br>\bigtriangledown J(\theta) = \frac{1}{2}(2X^TX\theta - (Y^TX)^T -X^TY)<br>$$</p><p>$$<br>=\frac{1}{2}(2X^TX\theta -X^TY-X^TY)<br>$$</p><p>$$<br>=X^TX\theta -X^TY<br>$$</p><p>令$\bigtriangledown J(\theta) =0$,可得：<br>$$<br>X^TX\theta = X^TY<br>$$<br>此时，如果$X^TX$可逆，则方程有最优解：<br>$$<br>\theta = (X^TX)^{-1}X^TY<br>$$<br>如果$X^TX$不可逆，一般加入干扰项，正则化处理：<br>$$<br>\theta = (X^TX + \lambda I)^{-1}X^TY<br>$$</p><h5 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h5><p>说完了最小二乘法的推导，我们再看看另外一种办法，梯度下降法的推导。</p><p>我们一般将损失函数写作（多加了个常数m，不影响结果）：<br>$$<br>J(\theta) = \frac{1}{2m} \sum_{j=1}^m (h_\theta(x^{(i)})- y^{(j)})^2<br>$$<br>直接对$\theta$求偏导：<br>$$<br>\frac{\partial}{\partial \theta_j} J(\theta) = \frac{\partial}{\partial \theta_j}\sum_{j=1}^m  \frac{1}{2m} (h_\theta(x^{(i)})- y^{(j)})^2<br>$$</p><p>$$<br>=\frac{1}{2m}2 \sum_{i=1}^m (h_\theta(x^{(i)})- y^{(j)}) \frac{\partial}{\partial \theta_j} \sum_{i=1}^m(\theta x^{(i)} - y^{(i)})<br>$$</p><p>$$<br>=\frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)})- y^{(j)}) \frac{\partial}{\partial \theta_j} \sum_{i=1}^m(\theta x^{(i)} - y^i)<br>$$</p><p>$$<br>= \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)})- y^{(j)}) \sum_{i=1}^m x_j^{(i)}<br>$$</p><p>$$<br>= \frac{1}{m} \sum_{i=1}^m ((h_\theta(x^{(i)})- y^{(j)}) x_j^{(i)})<br>$$</p><p>加入学习率$\alpha$,可得：<br>$$<br>\theta_i := \theta_i-\alpha \frac{\partial}{\partial \theta_j} J(\theta)<br>$$</p><p>然后我们可以选择具体选择哪种梯度下降法：</p><ul><li><strong>批量梯度下降法</strong>是最原始的形式，它是指在每一次迭代时使用所有样本来进行梯度的更新。</li><li><strong>随机梯度下降法</strong>不同于批量梯度下降，随机梯度下降是每次迭代使用一个样本来对参数进行更新。使得训练速度加快。</li><li><strong>小批量梯度下降</strong>，是对批量梯度下降以及随机梯度下降的一个折中办法。其思想是：每次迭代 使用 batch_size个样本来对参数进行更新。</li></ul><h4 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h4><h5 id="模型公式-1"><a href="#模型公式-1" class="headerlink" title="模型公式"></a>模型公式</h5><p>它的模型公式如下：<br>$$<br>h_\theta(x) = g(\theta^Tx)=\frac{1}{1+e^{- \theta^TX}}<br>$$<br>而它又复合伯努利分布，所以：<br>$$<br>p(Y=y|x)= \begin{cases}<br>h_\theta(x) = g(f(x)) = \frac{1}{1+e^{- \theta^TX}}, y=1 \\<br>1- h_\theta(x) = 1 - g(f(x)) = \frac{e^{- \theta^TX}}{1+e^{- \theta^TX}}, y=0<br>\end{cases}<br>$$<br>对数损失函数的标准形式为：<br>$$<br>L(Y,P(Y|X)) = -log(Y|X)<br>$$<br>它的意思是指分类为Y的情况下，使P(Y|X)达到最大。若模型是用最大概率的分类来做预测的，而Y是代表分类为正确的分类，而P(Y|X)则是代表正确分类的概率，那对数取反就是P(Y|X)越大，损失函数就越小。P(Y|X)=1时，损失就降为0，不可能再低了。 </p><p>将逻辑回归的表达式带入对数损失函数中，可得：<br>$$<br>L(y,P(Y=y|X)= \begin{cases}<br>log(h_\theta(x)), y=1 \\<br>log(1-h_\theta(x)), y=0<br>\end{cases}<br>$$<br>化简之后可得最终的损失函数为：<br>$$<br>J(\theta) = - \frac{1}{m} \sum_{i=1}^m[y_ilog(h_\theta(x_i))+(1-y_i)log(1-h_\theta(x_i))]<br>$$</p><h5 id="梯度下降法-1"><a href="#梯度下降法-1" class="headerlink" title="梯度下降法"></a>梯度下降法</h5><p>$$<br>\frac{\partial}{\partial \theta_j} J(\theta) = - \frac{1}{m} \sum_{i=1}^m(y^{(i)} \frac{1}{h_\theta(x^{(i)})} \frac{\partial h_\theta(x^{(i)})}{\partial \theta_j} - (1-y^{(i)}) \frac{1}{1-h_\theta(x^{(i)})} \frac{\partial h_\theta(x^{(i)})}{\partial \theta_j})<br>$$</p><p>$$<br>=- \frac {1}{m} \sum_{i=1}^m(y^{(i)} \frac{1}{g(\theta^Tx)}-(1-y^{(i)}) \frac{1}{1-g(\theta^Tx)}) \frac{\partial g(\theta^Tx)}{\partial \theta_j}<br>$$</p><p>因为:<br>$$<br>g(x) = \frac {1}{1+e^{-x}}<br>$$</p><p>$$<br>g’(x)=g(x)(1-g(x))<br>$$</p><p>所以上式继续化简可得：<br>$$<br>= - \frac{1}{m} \sum_{i=1}^m (y^{(i)}\frac{1}{g(\theta^Tx)}-(1-y^{(i)}) \frac{1}{1-g(\theta^Tx)}) g(\theta^Tx^{(i)})(1-g(\theta^Tx^{(i)})) x_j^{(i)}<br>$$</p><p>$$<br>=- \frac{1}{m} \sum_{i=1}^{m}(y^{(i)}(1-g(\theta^Tx^{(i)}))-(1-y^{(i)})g(\theta^Tx^{(i)}))x_j^{(i)}<br>$$</p><p>$$<br>=- \frac{1}{m} \sum_{i=1}^{m}(y^{(i)}-g(\theta^Tx^{(i)}))x_j^{(i)}<br>$$</p><p>$$<br>= \frac{1}{m} \sum_{i=1}^{m}(h_\theta(x^{(i)}) -y^{(i)})x_j^{(i)}<br>$$</p><p>是不是觉得和线性回归的有点类似，形式确实一样，但是$h_\theta(x)$是不一样的。</p><p>接下来采用合适的梯度下降策略，一次次的迭代直到找到最优解。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 线性回归 </tag>
            
            <tag> 逻辑回归 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>这！就是深度学习之机器学习基础(三十八)</title>
      <link href="/2019/08/19/dl38/"/>
      <url>/2019/08/19/dl38/</url>
      
        <content type="html"><![CDATA[<p>  这次主要讲降维的二种方法：主成分分析和线性判别分析的公式推导。</p><h5 id="主成分分析（Principal-Component-Analysis，简称PCA）"><a href="#主成分分析（Principal-Component-Analysis，简称PCA）" class="headerlink" title="主成分分析（Principal Component Analysis，简称PCA）"></a>主成分分析（Principal Component Analysis，简称PCA）</h5><p>  是一种降维方式，属于无监督学习。举个例子，预测房价的时候，我们会有许多特征，比如房子的长，宽，高，面积，体积，等等。但是这些特征中，房子面积可以由长乘宽得到，房子体积可以由长乘宽乘高得到，所以面积和体积这些特征，和长，宽，高这些特征是线性相关的，所以PCA就是通过去除线性相关来达到降维的目的。</p><p>  PCA要找的是主成分的轴的方向，根据的是最大投影方差和最小重构距离。最大投影方差可以理解为将样本点投影到主轴之后，分散的最开，即方差最大。最小重构误差，则是要求投影的距离最小。举个简单的例子，如下图所示：<br> <img src="/2019/08/19/dl38/dl3801.jpg" alt="二维降一维"><br>  在二维坐标系中，所有样本点在$u_1$方向，不管是投影之后的误差，还是投影的距离，都要比$u_2$好，这里的好就体现在2个方面，即样本点在$u_1$投影之后尽可能的分散开了，并且样本点到$u_1$的距离很近，所以我们认为$u_1$就是降维的主成分方向。</p><p>  不管用哪种方法，都要首先对数据进行中心化，就是把数据平移，从而使所有的数据中心为0，这样，当找到一个特征向量a=[1,2], 主成分的方向就是从原点出发，到点(1,2)的向量方向，否则难以界定何为主方向。</p><h6 id="基于最大投影方差"><a href="#基于最大投影方差" class="headerlink" title="基于最大投影方差"></a>基于最大投影方差</h6><p><img src="/2019/08/19/dl38/dl3802.jpg" alt="向量投影"><br>假设有n个中心化之后的样本$X=\{x_1,x_2,\cdots,x_n\}$, 此时的主方向所在的向量为$\vec u$, 并且u为单位向量，即$\left | u\right |=1$，如上图所示，对于其中任意一个样本点来说，则投影在u向量之后的向量为<br>$$\vec x \cdot \vec u$$<br>用矩阵表示就是：<br>$$x^Tu$$<br>因为已经中心化了，所以它的方差为:<br>$$\left | x^Tu \right |^2 = (x^Tu)^T(x^Tu)=(u^Tx)(x^Tu)$$<br>又因为矩阵的乘法结合律，所以上式<br>$$=u^T(xx^T)u$$<br>对于所有的样本点来说，它们的方差和为：<br>$$\frac{1}{n} \sum_{i=1}^n u^T(x_ix_i^T)u$$<br>$$=u^T (\frac{1}{n}  \sum_{i=1}^n (x_ix_i^T) )u$$<br>同时令$S=\frac{1}{n}  \sum_{i=1}^n (x_ix_i^T)$, 因为已经中心化了，所以这里的$x_i=x_i - \overline x$, 其实S就是协方差矩阵。<br>所以上式可以变为求<br>$$arcmax \sum_{i=1}^Nu^TSu$$<br>$$s.t. \left | u\right |=1$$<br>这是一个等式约束， 所以构造拉格朗日函数：<br>$$L(u,\lambda)=u^TSu +\lambda (1- \left | u\right |)$$<br>$$=u^TSu + \lambda (1- u^Tu)$$<br>然后对u求导：<br>$$\frac{\partial L}{\partial u} =2Su-\lambda 2u=0 $$<br>$$\Rightarrow Su = \lambda u$$<br>对于上式，其实就是特征值和特征向量的定义， $\lambda$是特征值组成的对角阵，$u$是特征向量组成的矩阵。特征值按照从大到小排列，前k个特征值对应的特征向量组成的矩阵即为我们降到k维对应的解。</p><h6 id="基于最小投影距离（最小重构距离）"><a href="#基于最小投影距离（最小重构距离）" class="headerlink" title="基于最小投影距离（最小重构距离）"></a>基于最小投影距离（最小重构距离）</h6><p>假设有n个中心化之后的样本$X=\{x_1,x_2,\cdots,x_n\}$, 此时的主方向所在的向量为$\vec u$, 并且u为单位向量，即$\left | u\right |=1$, 对于其中任意一个样本点来说，则投影的向量$\vec e$为：<br>$$\vec e = \vec x - (\vec x \cdot \vec u)\vec u$$<br>用矩阵表示就是：<br>$$\vec e=x-(x^Tu)u$$</p><p>那模长就是：<br>$$\left | \vec e\right |^2 = e^Te=[x-(x^Tu)u]^T[x-(x^Tu)u]$$<br>这里的$x^Tu$是向量的点积，得到的结果为一个实数，所以上式可以变为：<br>$$=[x^T-(x^Tu)u^T][x-(x^Tu)u]$$<br>$$=x^Tx-(x^Tu)u^Tx-(x^Tu)x^Tu+(x^Tu)^2 u^Tu$$<br>$$=x^Tx-(x^Tu)x^Tu-(x^Tu)x^Tu+(x^Tu)^2$$<br>$$=x^Tx-(x^Tu)(x^Tu)-(x^Tu)(x^Tu)+(x^Tu)^2$$<br>$$=x^Tx-(x^Tu)^2-(x^Tu)^2+(x^Tu)^2$$<br>$$=\left | x\right |^2-(x^Tu)^2$$<br>因为$\left | x\right |^2$是固定值，所以我们要求上式的最小值，即求$(x^Tu)^2$的最大值。<br>又因为矩阵乘法的结合律<br>$$(x^Tu)^2=(x^Tu)(x^Tu)$$<br>$$=(u^Tx)(x^Tu)$$<br>$$=u^T(xx^T)u$$<br>刚才求的是一个样本点的距离，那对于所有的样本来说，我们现在的目标就是:<br>$$max \sum_{i=1}^Nu^T(x_ix_i^T)u = arcmax \sum_{i=1}^Nu^T(X)u$$<br>$$s.t. \left | u\right |=1$$<br>这个问题就变成了一个等式约束的优化问题，那我们构造拉格朗日函数：<br>$$L(u,\lambda)=u^TXu + \lambda (1- \left | u\right |)$$<br>$$=u^TXu + \lambda (1- u^Tu)$$<br>然后对u求导：<br>$$\frac{\partial L}{\partial u} =Xu-\lambda u=0 $$<br>$$\Rightarrow Xu = \lambda u$$</p><p>可以看到和最大投影方差的解的形式是一样的。</p><h5 id="线性判别分析法（Linear-Discriminant-Analysis-简称LDA）"><a href="#线性判别分析法（Linear-Discriminant-Analysis-简称LDA）" class="headerlink" title="线性判别分析法（Linear Discriminant Analysis, 简称LDA）"></a>线性判别分析法（Linear Discriminant Analysis, 简称LDA）</h5><p>是一种降维方式，属于监督学习。LDA希望选取一个投影方向，能够最大化的分离出原有类别。即期望类间样本方差小，不同类之间中心距离要大，如下图所示：<br><img src="/2019/08/19/dl38/dl3803.jpg" alt="LDA"></p><p>假设样本共K类，每一类的样本个数为$N_1, N_2, \cdots,N_k$, 则第k类表示为$x_k^1,x_k^2,\cdots,x_k^k$。<br>设$\tilde {x_i^j}$为${x_i^j}$投影后的坐标，则：<br>$$\tilde {x_i^j}=&lt;x,u&gt;u = (x^Tu)u$$<br>向量u为投影的方向，不一定为单位向量，可设为$\left | \vec u\right |^2 = u^Tu = a$, $\tilde m$为均值<br>则第k类的样本平方和为：<br>$$S_k=\sum_{\tilde {x} \in D_k} (\tilde x - \tilde m)^T(\tilde x - \tilde m)$$<br>$$=\sum_{x \in D_k} ((x^Tu)u-(m^Tu)u)^T ((x^Tu)u-(m^Tu)u)$$<br>$$=\sum_{x \in D_k} ((x^Tu)u^T-(m^Tu)u^T) ((x^Tu)u-(m^Tu)u)$$<br>$$=\sum_{x \in D_k} (x^Tu)^2u^Tu - (m^Tu)(x^Tu)u^Tu -(x^Tu)(m^Tu)u^Tu+(m^Tu)^2u^Tu$$<br>$$=a\sum_{x \in D_k}((x^Tu)^2 - 2(x^Tu)(m^Tu) + (m^Tu)^2)$$</p><p>样本方差为上式除以样本个数：<br>$$\frac {S_k} {N_k} =a (\frac {\sum_{x \in D_k} (x^Tu)^2}{N_k} - 2 \frac{\sum_{x \in D_k} x^T(um^Tu)}{N_k} +\frac {\sum_{x \in D_k} (m^Tu)^2} {N_k})$$<br>$$=a(\frac{\sum_{x \in D_k} (u^Tx)(x^Tu)}{N_k} -2 \frac {\sum_{x \in D_k} x^T} {N_k} (um^Tu) +(m^Tu)^2)$$<br>$$=a(u^T \frac{\sum_{x \in D_k} xx^T}{N_k}u -2m^T(um^Tu) + (m^Tu)^2)$$<br>$$=a(u^T \frac{\sum_{x \in D_k} xx^T}{N_k}u -2(m^Tu)(m^Tu) + (m^Tu)^2)$$<br>$$=a(u^T \frac{\sum_{x \in D_k} xx^T}{N_k}u - (m^Tu)^2)$$<br>$$=a(u^T \frac{\sum_{x \in D_k} xx^T}{N_k}u - (u^Tm)(m^Tu))$$<br>$$=a(u^T \frac{\sum_{x \in D_k} xx^T}{N_k}u - u^T(mm^T)u)$$<br>$$=au^T(\frac{\sum_{x \in D_k} xx^T}{N_k} - mm^T)u$$</p><p>上面某一类的样本方差，那么所有类别的样本方差总和就是：<br>$$\sum_{k=1}^K S_k =a \sum_{k=1}^K u^T(\frac {\sum_{k=1}^K x_k x_k^T}{N_k} - m^km_k^T)u$$<br>$$=au^T(\sum_{k=1}^K \frac {\sum_{k=1}^K x_k x_k^T}{N_k} - m^km_k^T)u$$<br>令<br>$$S_w=(\sum_{k=1}^K \frac {\sum_{k=1}^K x_k x_k^T}{N_k} - m^km_k^T)$$<br>可以看到$S_w$其实就是样本协方差矩阵，则上式可以表示为：<br>$$au^T S_w u$$<br>我们期望上式可以最小。</p><p>假设$\tilde {m_i}$表示投影之后的第i类样本的均值，$\tilde {m_j}$表示投影之后的第j类样本的均值，则不同类别之间的中心距离为：<br>$$S_{ij}=(\tilde {m_i} - \tilde {m_j})^T(\tilde {m_i} - \tilde {m_j})$$<br>$$=((m_i^Tu)u-(m_j^Tu)u)^T ((m_i^Tu)u-(m_j^Tu)u)$$<br>$$=((m_i^Tu)u^T-(m_j^Tu)u^T) ((m_i^Tu)u-(m_j^Tu)u)$$<br>$$=(m_i^Tu)^2u^Tu -(m_j^Tu)(m_i^Tu)u^Tu-(m_i^Tu)(m_j^Tu)u^Tu-(m_j^Tu)^2u^Tu$$<br>$$=u^T((m_i^Tu)^2 - 2(m_i^Tu)(m_j^Tu) + (m_j^Tu)^2)u$$<br>$$=u^T((m_i^T)^2u^2-2m_i^Tm_j^Tu^2+(m_j^T)^2u^2)u$$<br>$$=u^2u^T((m_i^T)^2-2m_i^Tm_j^T+(m_j^T)^2)u$$<br>$$=u^2u^T(m_i^T-m_j^T)^2u$$<br>$$=u^2u^T(m_i^T-m_j^T)^T(m_i^T-m_j^T)u$$<br>$$=u^2u^T(m_i-m_j)(m_i-m_j)^Tu$$<br>对于所有不同类的中心距离之和为：<br>$$\sum_{i \neq j}^K S_{ij}= \sum_{i \neq j}^K u^2u^T(m_i-m_j)(m_i-m_j)^Tu$$<br>$$=u^2u^T(\sum_{i \neq j}^K (m_i-m_j)(m_i-m_j)^T)u$$<br>令$S_b=(\sum_{i \neq j}^K (m_i-m_j)(m_i-m_j)^T$,则上式可写为：<br>$$u^2u^TS_bu$$</p><p>我们令<br>$$J(u)=\frac{u^2u^TS_bu}{au^T S_w u}$$<br>因为前面说了$\left | \vec u\right |^2 = u^Tu = a$，所以上式变为：<br>$$J(u)=\frac{u^TS_bu}{u^T S_w u}$$<br>我们希望J(u)最大，也就是希望$u^TS_bU$最大，$u^T S_w u$最小，我们假定$u^T S_w u=1$,则我们的目标就变为了：<br>$$max u^TS_bu$$<br>$$s.t. \    u^T S_w u=1$$<br>有变成了一个等式约束，我们应当很熟悉了，利用拉格朗日乘子法求解，首先构造拉格朗日函数：<br>$$L(u, \lambda)= u^TS_bu + \lambda(1-u^T S_w u)$$<br>求导，令导数为零：<br>$$\frac {\partial L}{\partial u} = 2S_bu-\lambda 2S_wu=0$$<br>$$\Rightarrow S_bu =\lambda S_wu$$<br>$$\Rightarrow S_b S_w^{-1} u =\lambda u$$<br>令$T =S_b S_w^{-1}$,则上式变为<br>$$Tu =\lambda u$$<br>又变成了求解特征值和特征向量的问题。</p><h5 id="PCA和LDA的区别"><a href="#PCA和LDA的区别" class="headerlink" title="PCA和LDA的区别"></a>PCA和LDA的区别</h5><ul><li>LDA是有监督的降维方法，而PCA是无监督的降维方法</li><li>LDA降维最多降到类别数k-1的维数，而PCA没有这个限制。</li><li>LDA除了可以用于降维，还可以用于分类。</li><li>LDA选择分类性能最好的投影方向，而PCA选择样本点投影具有最大方差的方向。这点可以从下图形象的看出，在某些数据分布下LDA比PCA降维较优。<br><img src="/2019/08/19/dl38/dl3804.jpg" alt="PCA和LDA的区别"></li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PCA </tag>
            
            <tag> LDA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>这！就是深度学习之机器学习基础(三十七)</title>
      <link href="/2019/08/19/dl37/"/>
      <url>/2019/08/19/dl37/</url>
      
        <content type="html"><![CDATA[<p>支持向量机主要用于二分类的问题，是一种监督学习算法。<br>比如说对于这样的一个分类问题：<br><img src="/2019/08/19/dl37/dl3701.jpg" alt="SVM"></p><p>我们期望找到一个超平面$w^Tx+b=0$,把样本可以正确的分类出来。离超平面最近的样本点称为支持向量，所以分类的原则就是支持向量所在的平行平面间隔最远。</p><h5 id="支持向量机求解"><a href="#支持向量机求解" class="headerlink" title="支持向量机求解"></a>支持向量机求解</h5><p>现在假设经过支持向量的平面分别为$w^Tx+b=C_1$和$w^Tx+b=-C_2$，因为是平行的平面，那么我们分别除以常数$C_1$和$C_2$,可得新的平面为$\frac{w^T}{C_1}x+\frac{b}{C_1}=1$和$\frac{w^T}{C_2}x+\frac{b}{C_2}=-1$, 既然系数都是常数,那么我们也可以写成新的$w^Tx+b=1$和$w^Tx+b=-1$。那么我们现在计算两个平面间隔的距离。<br>首先任意一个点到超平面的距离为$\gamma = \frac{\vert w^Tx+b+0 \vert}{\Vert w \Vert}$,其中w为法向量，我们现在要求最小的$\gamma$, 由上面的图可以看出来，对于任何一个正例样本点，都有$w^T+b \geq 1$, 对于任意一个负例样本点，都有$w^T+b \leq 1$, 加上绝对值之后，最小的距离就是1了，所以最近点的，即支持向量到超平面的距离就是$\gamma = \frac{1}{\Vert w \Vert}$，要让两个平面的间隔最大，就转化为了求$max \frac{2}{\Vert w \Vert}$ ，就让分子最小化：<br>$$\underset {w,b}{min} \frac{1}{2} \ {\Vert w \Vert}^2$$<br>$$s.t. y_i(w^Tx_i + b) \geq 1, i=1,2, \cdots,N$$</p><p>前面加了$\frac{1}{2}$, 只是为了后面求导可以方面一些。</p><p>对于上面不等式约束的优化问题，首先我们引入拉格朗日函数，因为拉格朗日函数可以把有约束的优化问题变为无约束的优化问题。定义拉格朗日乘子为$\lambda_i, \lambda_i \geq 0$，则拉格朗日函数为：<br>$$L(w,b,\lambda) = \frac{1}{2} {\Vert w \Vert}^2 + \sum_{i=1}^{N} \lambda_i (1- y_i(w^Tx_i + b))$$<br>这样就相当于原始的约束条件消掉了, 融进了新的拉格朗日函数。<br>通过上式可以看到，如果对于任意一个样本点不符合约束$y_i(w^Tx_i + b) &lt; 1$,则$(1- y_i(w^Tx_i + b)) &gt;0$, 又因为$\lambda_i \geq 0$, 所以$L(w,b,\lambda) = \frac{1}{2}  {\Vert w \Vert}^2 + \infty = \infty$,没有解。<br>如果所有样本点符合约束，同样的道理，$L(w,b,\lambda)$就有极大值$\frac{1}{2} {\Vert w \Vert}^2$<br>这就是我们期望的, 也就是我们现在的问题就是求解<br>$$\underset{w,b}{min} \underset{\lambda_i \geq 0}{max} L(w,b,\lambda)$$<br>因为是一个凸二次规划问题，满足强对偶性，所以极大极小可以转为极小极大问题：<br>$$\underset{\lambda_i \geq 0}{max} \underset{w,b}{min}  L(w,b,\lambda)$$</p><p>首先求$\underset{w,b}{min}L(w,b,\lambda)$, 分别求w和b的偏导数，并且令偏导数为零即可求得最小值。<br>$$\frac{\partial L(w,b,\lambda)}{\partial w} = w - \sum_{i=1}^N \lambda_i y_i x_i = 0\  \Rightarrow w=\sum_{i=1}^N \lambda_i y_i x_i$$<br>$$\frac{\partial L(w,b,\lambda)}{\partial b} = 0 + \sum_{i=1}^N \lambda_i y_i=0\  \Rightarrow \sum_{i=1}^N \lambda_i y_i =0$$</p><p>把上式中的w和b带回拉格朗日函数$L(w,b,\lambda)$ :<br>$$L(w,b,\lambda) =\frac{1}{2}w^Tw+\sum_{i=1}^N(\lambda_i-\lambda_i y_i w^T x_i + \lambda_i y_i b)<br>$$<br>$$=\frac{1}{2} \sum_{i=i}^N \sum_{j=i}^N \lambda_i \lambda_j y_i y_j (x_i \cdot x_j) + \sum_{i=1}^N \lambda_i -\sum_{i=1}^N \sum_{j=1}^N \lambda_i \lambda_j y_i y_j(x_i \cdot x_j) + \sum_{i=1}^N \lambda_i y_i b<br>$$<br>因为$\sum_{i=1}^N \lambda_i y_i =0$, 所以上式：<br>$$= \sum_{j=i}^N \lambda_i - \frac{1}{2} \sum_{i=i}^N \sum_{j=i}^N \lambda_i \lambda_j y_i y_j (x_i \cdot x_j)<br>$$</p><p>接着求$\underset{\lambda_i \geq 0}{max} L(w,b,\lambda)$, 此时改变一下符号可变为求最小值：<br>$$\underset{\lambda_i \geq 0}{min} \ \frac{1}{2} \sum_{i=i}^N \sum_{j=i}^N \lambda_i \lambda_j y_i y_j (x_i \cdot x_j) -\sum_{j=i}^N \lambda_i $$<br>$$s.t. \sum_{i=1}^N \lambda_i y_i =0$$<br>$$\lambda_i \geq 0, i =1, 2, \cdots,N$$</p><p>当求出最大的$\lambda_i$，也就求出了w，b，从而也就确定了超平面的函数：<br>$$w^Tx+b = \sum_{i=1}^N \lambda_iy_i x_i^Tx+b$$<br>需要满足的KKT条件是:<br>$$\begin{cases}<br>\lambda_i \geq 0, i= 1,2,\cdots,N\<br>y_i(w^Tx+b)-1 \geq 0\<br>\lambda_i(y_i(w^Tx+b)-1) = 0\<br>\end{cases}<br>$$<br>通过上式，可以看到，如果$\lambda_i &gt;0$,则必有$y_i(w^Tx+b)-1=0$,此时对应的样本就是支持向量。</p><p>对于$\lambda_i$的求法，因为一次算出所有的$\lambda_i$值不太容易，我们可以用SMO（Sequential Minimal Optimization, 序列最小化）算法求得最大的$\lambda_i$.</p><p>SMO算法就是一次比较2个$\lambda_i$的值，将其他的变量都视为常数，每次保留其中最大的值，依次比较，从而求出最大的$\lambda_i$。</p><h5 id="软间隔"><a href="#软间隔" class="headerlink" title="软间隔"></a>软间隔</h5><p>因为在实际情况中，一个面把所有点都100%的分割出来是比较难的，所以我们允许一些误差出现，此时引入松弛向量$\xi$，我们称之为软间隔，则原始问题就变成了：<br>$$\underset {w,b}{min} \frac{1}{2} \ {\Vert w \Vert}^2 + C \sum_{i=1}^N \xi_i$$<br>$$s.t. \  y_i(w^Tx_i + b) \geq 1-\xi_i, i=1,2, \cdots,N$$<br>$$\xi_i \geq 0, i = 1,2, \cdots,N $$</p><p>C是常数，表示犯错误的容忍程度，C越大，表示容忍度越低，即过渡带会越小。反之，越大。</p><p>求解方式也是类似的，引入拉格朗日乘子$\alpha_i,\mu$，转化为拉格朗日函数：<br>$$L(w,b,\xi,\alpha,\mu) = \frac{1}{2} {\Vert w \Vert}^2 + C \sum_{i=1}^N \xi_i + \sum_{i=1}^{N} \alpha_i (1-\xi_i- y_i(w^Tx_i + b)) - \mu \xi_i$$<br>分别求偏导可得：<br>$$\frac{\partial L(w,b,\xi,\alpha,\mu)}{\partial w} =0 \Rightarrow w=\sum_{i=1}^N \lambda_i y_i x_i$$<br>$$\frac{\partial L(w,b,\xi,\alpha,\mu)}{\partial b} = 0 \Rightarrow \sum_{i=1}^N \lambda_i y_i =0$$<br>$$\frac{\partial L(w,b,\xi,\alpha,\mu)}{\partial \xi} = 0 \Rightarrow C- \alpha_i - \mu_i = 0$$<br>把上式带入原函数,可得:<br>$$L(w,b,\xi,\alpha,\mu)=-\frac{1}{2} \sum_{i=i}^N \sum_{j=i}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j) +\sum_{j=i}^N \lambda_i$$<br>现在的问题就变成了：<br>$$\underset{\alpha}{max} -\frac{1}{2} \sum_{i=i}^N \sum_{j=i}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j) +\sum_{j=i}^N \alpha_i$$<br>$$s.t. \ \sum_{i=1}^N \alpha_i y_i =0$$<br>$$C-\alpha_i-\mu_i = 0$$<br>$$\alpha_i \geq 0, i = 1,2,\cdots,N$$<br>$$\mu_i \geq 0, i = 1,2,\cdots,N$$<br>因为$0 \leq \alpha_i \geq C$, 同时求最大变为求最小，所以上式可变为：<br>$$\underset{\alpha}{min} \ \frac{1}{2} \sum_{i=i}^N \sum_{j=i}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j) -\sum_{j=i}^N \alpha_i$$<br>$$s.t. \ C-\alpha_i-\mu_i = 0$$<br>$$0 \leq \alpha_i \geq C, i = 1,2,\cdots,N$$</p><p>此时的KKT条件为：<br>$$\begin{cases}<br>\lambda_i \geq 0, \mu_i \geq 0, i= 1,2,\cdots,N \\<br>y_i(w^Tx+b)-1+\xi_i \geq 0 \\<br>\lambda_i(y_i(w^Tx+b)-1+\xi_i) = 0 \\<br>\xi_i \geq 0, \mu_i \xi_i =0,i= 1,2,\cdots,N \\<br>\end{cases}<br>$$</p><h5 id="一般情况下的优化方式："><a href="#一般情况下的优化方式：" class="headerlink" title="一般情况下的优化方式："></a>一般情况下的优化方式：</h5><p>1 无约束优化问题，可以写为:<br>$min f(x)$<br>优化方法：根据费马大定律，求f(x)的导数，并且令导数为0.</p><p>2 有等式约束的优化问题，可以写为:<br>$min f(x)$<br>$s.t. h_i(x) = 0; i =1, …, n$<br>优化方法： 拉格朗日乘子法，通过拉格朗日函数把等式约束与原函数合为以个函数表示$L(x, \alpha) = f(x) + \alpha h_i(x)$, 然后对各个变量求导，令其为零，在结果集合中选取最优值。</p><p>3 有不等式约束的优化问题，可以写为：<br>$min f(x)$<br>$s.t. g_i(x) \leq 0, i =1,2,\cdots, n$<br>$h_i(x) = 0, i =1,2,\cdots, m$<br>优化方法： KKT条件。同样利用拉格朗日函数把所有的优化条件和原函数合为一个函数。$L(x,\alpha,\lambda) = f(x) + \lambda g_i(x) + \alpha h_i(x)$,此时最优值必须满足以下条件：<br>$$\begin{cases}<br>L(x,\alpha,\lambda) \text {对}x \text{求导为0} \\<br>h_i(x) = 0 \\<br>\lambda g_i(x) = 0 \\<br>\end{cases}<br>$$</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SVM </tag>
            
            <tag> KKT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>这！就是深度学习之机器学习基础(三十六)</title>
      <link href="/2019/08/11/dl36/"/>
      <url>/2019/08/11/dl36/</url>
      
        <content type="html"><![CDATA[<p>首先更新一下机器学习算法的思维导图：<br><img src="/2019/08/11/dl36/dl3601.jpg" alt="机器学习算法"><br>以前已经说过了的，就不再赘述，只是对部分的知识点，深挖一下，这次就先说极大似然估计（Maximum Likelihood Estimation）。</p><h5 id="极大似然估计的目的"><a href="#极大似然估计的目的" class="headerlink" title="极大似然估计的目的"></a>极大似然估计的目的</h5><p>利用已知样本结果，反推出最有可能（最大概率）导致这样结果的参数值。</p><h5 id="极大似然估计的原理"><a href="#极大似然估计的原理" class="headerlink" title="极大似然估计的原理"></a>极大似然估计的原理</h5><p>基于统计学中的概率论，通过若干次试验，观察其结果，利用实验结果得到某个参数值能够使样本出现的概率最大。</p><h6 id="1-似然函数"><a href="#1-似然函数" class="headerlink" title="1. 似然函数"></a>1. 似然函数</h6><p>假设样本集中的样本都是独立同分布，记为：D={$x_1, \cdots,x_N$}, 则其概率密度函数$p\left(D|\theta\right)$称为在给定数据D的情况下，参数θ的似然函数：<br>$$l(θ)=p(D|θ)=p(x_1, \cdots,x_N)=\prod_{i=1}^{N}p(x_i|θ)$$<br>如果$\hat{\theta}$是使得似然函数l(θ)最大的θ值，则称θ为极大似然函数估计值，即<br>$$\hat{\theta}=\underset{\theta}{argmax}l(\theta)=\underset{\theta}{argmax}\prod_{i=1}^{N}p(x_i|θ)$$</p><h6 id="2-对数似然函数"><a href="#2-对数似然函数" class="headerlink" title="2. 对数似然函数"></a>2. 对数似然函数</h6><p>为了便于求导，我们一般取对数似然函数$Inl\left(\theta\right)$，这样可以把乘法转换成加法，而且不改变它的单调性，我们假设新的对数似然函数为H(θ)则：<br>$$H(θ)=Inl(θ)=ln\prod_{i=1}^{N}p(x_i|θ)=\sum_{i=1}^{N}lnp(x_i|θ)$$</p><h6 id="3-求解"><a href="#3-求解" class="headerlink" title="3. 求解"></a>3. 求解</h6><p>令其导数为零，$\frac{d}{dx}H(θ)=0$</p><h5 id="极大似然估计的应用："><a href="#极大似然估计的应用：" class="headerlink" title="极大似然估计的应用："></a>极大似然估计的应用：</h5><p>要想知道极大似然估计可以用在哪里，我们关键要知道概率密度函数$p\left(D|\theta\right)$有哪些，举2个常见的例子。</p><h6 id="1-高斯分布"><a href="#1-高斯分布" class="headerlink" title="1. 高斯分布"></a>1. 高斯分布</h6><p>假设样本服从$N(\mu,\sigma^{2})$, 则高斯分布为：<br>$$N(x|\mu,\sigma^{2})=\frac{1}{\sqrt{2\pi}\sigma}exp \left (\frac{1}{2\sigma^{2}}\left (x-\mu \right)^{2}\right )$$<br>其似然函数为：<br>$$L(\mu,\sigma^{2})= \prod_{i=1}^{N}\frac{1}{\sqrt{2\pi}\sigma}exp \left (\frac{1}{2\sigma^{2}}\left (x-\mu \right)^{2}\right )$$<br>其对数似然函数为：<br>$$InL(\mu,\sigma^{2}) = In\prod_{i=1}^{N}\frac{1}{\sqrt{2\pi}\sigma}exp \left (\frac{1}{2\sigma^{2}}\left (x-\mu \right)^{2}\right ) $$<br>$$ = \sum_{i=1}^NIn\frac{1}{\sqrt{2\pi}\sigma}exp \left (\frac{1}{2\sigma^{2}}\left (x-\mu \right)^{2}\right ) $$<br>$$ = \sum_{i=1}^NIn\frac{1}{\sqrt{2\pi}\sigma} + \sum_{i=1}^N-\frac{\left (x-\mu \right)^2}{2\sigma^2}$$<br>$$ = -\frac{n}{2}In(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^N(x-\mu)^2$$<br>$$ = -\frac{n}{2}In(2\pi)  -\frac{n}{2}In(\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^N(x-\mu)^2 $$<br>要求似然函数最大值，分别对$\mu,\sigma^2$求偏导，并且令导数为零：<br>$$\left\{\begin{matrix}<br>\frac{h(x)}{d\mu} = 0 + \frac{1}{\sigma^2}\sum_{i=1}^n(x_{i}-\mu) = 0 \\<br>\frac{h(x)}{d\sigma^2} = -\frac{n}{2\sigma^2} + \frac{1}{2\sigma^4}\sum_{i=1}^n(x_{i}-\mu)^2 = 0<br>\end{matrix}\right.$$<br>从而求得$\mu,\sigma^2$为：<br>$$\left\{\begin{matrix}<br>\mu = \bar{x} = \frac{1}{n}\sum_{i=1}^{n}x_i \\<br>\sigma^{2} = \frac{1}{n}\sum_{i=1}^{n}(x_i-\bar {x})^2<br>\end{matrix}\right.$$</p><p>似然函数有唯一解$(\mu ,\sigma^{2})$, 所以它就是最大值点。</p><h6 id="2-伯努利（Bernoulli）分布"><a href="#2-伯努利（Bernoulli）分布" class="headerlink" title="2. 伯努利（Bernoulli）分布"></a>2. 伯努利（Bernoulli）分布</h6><p>伯努利分布是一个离散型机率分布。试验成功，随机变量取值为1；试验失败，随机变量取值为0。成功机率为p，失败机率为q =1-p，所以伯努利分布又称两点分布。<br>Bernoulli分布的密度函数为：<br>$$Ber(x|θ)=\theta^x(1-\theta)^{1-x}$$<br>其对数似然函数为：<br>$$l(θ)=\sum_{i=1}^{N}logBer(x_i|\theta)$$<br>$$ =\sum_{i=1}^{N}log(\theta^x(1-\theta)^{1-x})) $$<br>$$ =N_1log\theta + N_2log(1-\theta) $$</p><p>其中$N_1$为实验结果为1的次数， $N_2$为实验结果为0的次数：<br>$$\left\{\begin{matrix}<br>N_1 = \sum_{i=1}^{N}x_i \\<br>N_2 = \sum_{i=1}^{N}(1-x_i)<br>\end{matrix}\right.$$</p><p>求导可得：<br>$$\frac {dl(\theta)}{d\theta} = \frac{N_1}{\theta} - \frac{N_2}{1-\theta} = 0$$<br>$$\Rightarrow \hat{\theta} = \frac {N_1}{N_1+N_2} = \frac {N_1}{N}$$</p><p>逻辑回归的本质其实就是伯努利分布下的极大似然估计。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 极大似然估计 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于Github+hexo+matery搭建个人博客</title>
      <link href="/2019/08/09/setup-blog/"/>
      <url>/2019/08/09/setup-blog/</url>
      
        <content type="html"><![CDATA[<h4 id="前置条件"><a href="#前置条件" class="headerlink" title="前置条件"></a>前置条件</h4><ol><li>已经安装了Node.js</li><li>已经安装了Git</li><li>已经有了GitHub的账号</li><li>已经在GitHub上配置好了SSH Key</li></ol><h4 id="安装过程"><a href="#安装过程" class="headerlink" title="安装过程"></a>安装过程</h4><ol><li>在GitHub上新建一个仓库，仓库的名字为username.github.io，这里的username一定要是你github账号的名字。我开始因为起了个别的名字，导致上传到GitHub之后，在浏览器中输入username.github.io，一直显示404</li><li>在本地创建同名的文件夹username.github.io，然后在此文件夹上右键Git Bash Here.</li><li>执行下列命令<blockquote><p>hexo init #初始化hexo<br>npm install #安装依赖包<br>npm install hexo-deployer-git –save #确保git部署<br>此时，你便完成了hexo博客的搭建，并且自带了默认的主题landspace</p></blockquote></li><li>执行下列编译命令<blockquote><p>hexo g</p></blockquote></li><li>启动service<blockquote><p>hexo s #便于本地调试<br>然后在浏览器中输入<a href="http://localhost:4000即可查看，默认的主题landsapce里还有一篇自带的文章：" target="_blank" rel="noopener">http://localhost:4000即可查看，默认的主题landsapce里还有一篇自带的文章：</a> Hello World</p></blockquote></li><li>你可以更换不同的主题，比如下载量很高的nextT：<blockquote><p><a href="https://github.com/theme-next/hexo-theme-next" target="_blank" rel="noopener">nextT</a></p><p>或者我用的matery主题：<br><a href="https://github.com/blinkfox/hexo-theme-matery/blob/develop/README_CN.md" target="_blank" rel="noopener">matery</a></p></blockquote></li></ol><p>下载的方法上面的post里都有。</p><ol start="7"><li><p>有2个重要的配置文件_config.yml,分别是hexo的系统配置文件和各个主题所特有的配置文件，我把我的系统配置文件贴出来供参考：</p><blockquote><p>D:\GitHubRepository\KnightChen2019.github.io_config.yml</p><p>#Hexo Configuration<br>##Docs: <a href="https://hexo.io/docs/configuration.html" target="_blank" rel="noopener">https://hexo.io/docs/configuration.html</a><br>##Source: <a href="https://github.com/hexojs/hexo/" target="_blank" rel="noopener">https://github.com/hexojs/hexo/</a></p><p>#Site<br>title: 星辰白衣<br>subtitle: 星辰白衣的博客<br>description: 放弃很容易，但坚持一定很酷！<br>keywords:<br>author: 星辰白衣<br>language: zh-CN<br>timezone: Asia/Shanghai</p><p>#URL<br>##If your site is put in a subdirectory, set url as ‘<a href="http://yoursite.com/child&#39;" target="_blank" rel="noopener">http://yoursite.com/child&#39;</a> and root as ‘/child/‘<br>url: <a href="http://knightchen.com">http://knightchen.com</a><br>root: /<br>permalink: :year/:month/:day/:title/<br>permalink_defaults:</p><p>#Directory<br>source_dir: source<br>public_dir: public<br>tag_dir: tags<br>archive_dir: archives<br>category_dir: categories<br>code_dir: downloads/code<br>i18n_dir: :lang<br>skip_render:</p><p>#Writing<br>new_post_name: :title.md #File name of new posts<br>default_layout: post<br>titlecase: false #Transform title into titlecase<br>external_link: true #Open external links in new tab<br>filename_case: 0<br>render_drafts: false<br>post_asset_folder: true<br>relative_link: false<br>future: true<br>highlight:<br> enable: true<br> line_number: true<br> auto_detect: false<br> tab_replace:</p><p>prism_plugin:<br> mode: ‘preprocess’    #realtime/preprocess<br> theme: ‘tomorrow’<br> line_number: false    #default false<br> custom_css:</p><p>#Home page setting<br>#path: Root path for your blogs index page. (default = ‘’)<br>#per_page: Posts displayed per page. (0 = disable pagination)<br>#order_by: Posts order. (Order by date descending by default)<br>index_generator:<br> path: ‘’<br> per_page: 12<br> order_by: -date</p><p>#Category &amp; Tag<br>default_category: uncategorized<br>category_map:<br>tag_map:</p><p>#Date / Time format<br>##Hexo uses Moment.js to parse and display date<br>##You can customize the date format as defined in<br>##<a href="http://momentjs.com/docs/#/displaying/format/" target="_blank" rel="noopener">http://momentjs.com/docs/#/displaying/format/</a><br>date_format: YYYY-MM-DD<br>time_format: HH:mm:ss</p><p>#Pagination<br>##Set per_page to 0 to disable pagination<br>per_page: 12<br>pagination_dir: page</p><p>#Extensions<br>##Plugins: <a href="https://hexo.io/plugins/" target="_blank" rel="noopener">https://hexo.io/plugins/</a><br>##Themes: <a href="https://hexo.io/themes/" target="_blank" rel="noopener">https://hexo.io/themes/</a><br>theme: matery</p><p>#Deployment<br>##Docs: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">https://hexo.io/docs/deployment.html</a><br>deploy:<br> type: git<br> repository: <a href="mailto:git@github.com" target="_blank" rel="noopener">git@github.com</a>:KnightChen2019/KnightChen2019.github.io.git<br> branch: master</p><p>#New add search function<br>#npm install hexo-generator-search –save<br>search:<br> path: search.xml<br> field: post</p><p>#support chinese pinyin link<br>#npm i hexo-permalink-pinyin –save<br>permalink_pinyin:<br> enable: true<br> separator: ‘-‘ #default: ‘-‘</p><p>#Support RSS<br>#npm install hexo-generator-feed –save<br>feed:<br> type: atom<br> path: atom.xml<br> limit: 20<br> hub:<br> content:<br> content_limit: 140<br> content_limit_delim: ‘ ‘<br> order_by: -date</p></blockquote></li><li><p>把内容推送到GitHub上，执行下列命令即可：</p><blockquote><p>hexo d</p></blockquote></li><li><p>写博客，在本地执行：</p><blockquote><p>hexo new ‘post’</p></blockquote></li></ol><p>会在D:\GitHubRepository\KnightChen2019.github.io\source_posts目录里生成一个post.md的文件，你在里面创作即可。</p><ol start="10"><li><p>每次有任何改动都需要执行</p><blockquote><p>hexo g<br>hexo s #optional, 本地调试用<br>hexo d</p></blockquote></li><li><p>买域名。目前为止，我们可以通过<br>username.github.io去访问自己的博客了，但是这个名字太长，有没有酷一点的名字呢？当然有，买个域名即可，我在腾讯云上买了个5年期的域名knightchen.com，290.</p></li><li><p>绑定域名，在自己的腾讯云的域名服务中，选择刚买的域名，然后添加如下的3条域名解析：<br><img src="/2019/08/09/setup-blog/DomainName.jpg" alt="域名解析"><br>同时在本地博客的source目录下新建一个CNAME文件，把你的域名填加进去。</p></li><li><p>添加评论配置模块，matery内置了许多评论模块，<br>Gitalk、Gitment、Valine 和 Disqus等。我选用了Gitalk, 在主题的配置模块中启用此功能即可。</p><blockquote><p>D:\GitHubRepository\KnightChen2019.github.io\themes\matery_config.yml</p><p>#the Gitalk config，default disabled<br>#Gitalk 评论模块的配置，默认为不激活<br>gitalk:<br> enable: true<br> owner: KnightChen2019<br> repo: KnightChen2019.github.io<br> oauth:<br>   clientId: f0ad985XXXXXXXXXXXX<br>   clientSecret: e69425e9629fbaf678XXXXXXXXXXXX<br> admin: KnightChen2019</p></blockquote></li></ol><p>上图中的clientId和clientSecret信息需要我们在下面的网站创建一个:</p><p><a href="https://github.com/settings/applications/new" target="_blank" rel="noopener">OAuth application</a></p><p>至此，初步的blog已经创建成功了,剩下的就看你的精雕细琢了。</p><h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><ol><li>搭建途中，参考了下面这位同学的教程：<br><a href="https://godweiyang.com/2018/04/13/hexo-blog/#toc-heading-26" target="_blank" rel="noopener">韦阳的博客</a></li><li>Hexo的官方文档：<br><a href="https://hexo.io/zh-cn/docs/" target="_blank" rel="noopener">Hexo Doc</a></li><li>matery在GitHub上的ReadMe：<br><a href="https://github.com/blinkfox/hexo-theme-matery/blob/develop/README_CN.md" target="_blank" rel="noopener">hexo-theme-matery</a></li></ol><h4 id="Issue"><a href="#Issue" class="headerlink" title="Issue"></a>Issue</h4><ol><li>本地图片无法显示。</li></ol><ul><li>重新安装插件hexo-asset-image<blockquote><p>npm install <a href="https://github.com/CodeFalling/hexo-asset-image" target="_blank" rel="noopener">https://github.com/CodeFalling/hexo-asset-image</a> –save</p></blockquote></li><li>执行命令<blockquote><p>hexo clean #清除public folder和database<br>hexo g<br>hexo s</p></blockquote></li><li>此时引用图片的格式为：<blockquote><p>…(test/a.jpg)<br>…(a.jpg)</p></blockquote></li></ul>]]></content>
      
      
      <categories>
          
          <category> 软件安装与配置 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 博客教程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2019/08/01/hello-world/"/>
      <url>/2019/08/01/hello-world/</url>
      
        <content type="html"><![CDATA[<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class=" language-bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class=" language-bash"><code class="language-bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code>$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code>$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
